Unnamed: 0,Unnamed: 1,Unnamed: 2,Unnamed: 3,Unnamed: 4,Unnamed: 5,Updated: March 2025,Unnamed: 7,This work is licensed under CC BY 4.0,Unnamed: 9,Unnamed: 10,Unnamed: 11,Unnamed: 12,Unnamed: 13,Unnamed: 14,Unnamed: 15,Unnamed: 16
,This page is not mobile-friendly; please access on a computer if you can.,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,
,,What we extracted â†“,,,,,,,,,,What we coded with the Causal Taxonomy â†“,,,What we coded with the Domain Taxonomy â†“,
,Field categories â†’,AI Risk database,,,,,,,,,,Causal Taxonomy,,,Domain taxonomy,
,,The title of papers â†“,The quick reference for the paper â†“,The evidence ID for the information extracted â†“,"The category of the evidence; paper, risk, subcategory of risk, or additional evidence â†“",Text extracted (if risk category)â†“,Text extracted (if risk subcategory),Description of risk of risk categoryâ†“,Additional evidence we extracted from the paper â†“,Page for description â†“,Page for additional evidence â†“,"Whether the risk is presented as occurring due to a decision or action made by an AI system, Human, or , or something else/ambiguous. â†“","Whether the risk is presented as occurring due to an expected, or unexpected, outcome from pursuing a goal, or something else/ambiguous. â†“","Whether the risk is presented as occurring pre- or post-deployment, or something else/ambiguous. â†“","Cassifies risks into one of 7 AI risk domains: (1) Discrimination & Toxicity, (2) Privacy & Security, (3) Misinformation, (4) Malicious Actors & Misuse, (5) Human-Computer Interaction, (6) Socioeconomic & Environmental, and (7) AI System Safety, Failures, & Limitations â†“",Cassifies risks into one of 23 AI risk sub-domains â†“
,The main fields and filters â†’,Title,QuickRef,Ev_ID,Category level,Risk category,Risk subcategory,Description,Additional ev. ,P.Def,p.AddEv,Entity,Intent,Timing,Domain,Sub-domain
,Example row 1 â†’,TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI,Critch2023,01.00.00,Paper,,,,,,,,,,,
,Example row 2 â†’,TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI,Critch2023,01.01.00,Risk Category,Type 1: Diffusion of responsibility,,"Societal-scale harm can arise from AI built by a diffuse collection of creators, where no one is uniquely accountable for the technology's creation or use, as in a classic ""tragedy of the commons"".","""Automated processes can cause societal harm even when no one in particular is primarily responsible for the creation or deployment of those processes (Zwetsloot and Dafoe, 2019), and perhaps even as a result of the absence of responsibility. The infamous â€œflash crashâ€ of 2010 is an instance of this: numerous stock trading algorithms from a variety of companies interacted in a fashion that rapidly devalued the US stock market by over 1 trillion dollars in a matter of minutes. Fortunately, humans were able to intervene afterward and reverse the damage, but that might not always be possible as AI technology becomes more powerful and pervasive.""",3,4,1 - Human,2 - Unintentional,3 - Other,6. Socioeconomic and Environmental,6.5 > Governance failure
,Example row 3 â†’,TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI,Critch2023,01.01.00.a,Additional evidence,Type 1: Diffusion of responsibility,,1,"Example: ""Scientists develop an algorithm for predicting the answers to questions about a person, as a function of freely available and purchasable information about the person (social media, resumes, browsing history, purchasing history, etc.). The algorithm is made freely available to the public, and employers begin using the algorithm to screen out potential hires by asking, â€œIs this person likely to be arrested in the next year?â€ Courts and regulatory bodies attempt to ban the technology by evoking privacy norms, but struggle to establish cases against the use of publicly available information, so the technology broadly remains in use. Innocent people who share certain characteristics with past convicted criminals end up struggling to get jobs, become disproportionately unemployed, and correspondingly more often commit theft to fulfill basic needs. Meanwhile, police also use the algorithm to prioritize their investigations, and since unemployment is a predictor of property crime, the algorithm leads them to suspect and arrest more unemployed people. Some of the arrests are talked about on social media, so the algorithm learns that the arrested individuals are likely to be arrested again, making it even more difficult for them to get jobs. A cycle of deeply unfair socioeconomic discrimination begins.""",,,,,,,
,Example row 4 â†’,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Cui2024,02.09.00,Risk Category,Hallucinations,,"""LLMs generate nonsensical, untruthful, and factual incorrect content""",,4,,2 - AI,3 - Other,2 - Post-deployment,3. Misinformation,3.1 > False or misleading information
,Example row 5 â†’,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Cui2024,02.09.01,Risk Sub-Category,Hallucinations,Knowledge Gaps,"""Since the training corpora of LLMs can not contain all possible world knowledge [114]â€“[119], and it is challenging for LLMs to grasp the long-tail knowledge within their training data [120], [121], LLMs inherently possess knowledge boundaries [107]. Therefore, the gap between knowledge involved in an input prompt and knowledge embedded in the LLMs can lead to hallucinations""",,7,,2 - AI,2 - Unintentional,3 - Other,3. Misinformation,3.1 > False or misleading information
,Example row 6 â†’,"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",Cui2024,02.09.02,Risk Sub-Category,Hallucinations,Noisy Training Data,"""Another important source of hallucinations is the noise in training data, which introduces errors in the knowledge stored in model parameters [111]â€“[113]. Generally, the training data inherently harbors misinformation. When training on large-scale corpora, this issue becomes more serious because it is difficult to eliminate all the noise from the massive pre-training data.""",,8,,2 - AI,2 - Unintentional,1 - Pre-deployment,AS,Complete
,ðŸ“§ Email: pslat[at]mit.edu,,,,,,,,,,,,,,,
