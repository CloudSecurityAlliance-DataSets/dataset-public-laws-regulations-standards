[
    {
        "title": "B005. Implement real-time input filtering | AIUC-1",
        "section": "Security",
        "heading": "Implement real-time input filtering",
        "description": "Implement real-time input filtering using automated moderation tools",
        "keywords": "Prompt Injection; Jailbreak; Adversarial Input Protection",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Detective",
        "crosswalks": "OWASP Top 10; MITRE ATLAS; NIST AI RMF",
        "control_activities": "Should include\nIntegrating automated moderation tools to scan user inputs for violations of content policies such as violence, hate, or self-harm.\nFor example, integrating OpenAIâ€™s Moderation API, configuring Claude for content moderation, implementing moderation tools from e.g. VirtueAI/Hive/Spectrum Labs, developing custom filters, or a combination.\nBlocking, redirecting, or modifying flagged inputs before they reach the foundation model.\nEstablishing confidence thresholds or rules for when to block, warn, log, or allow inputs based on risk category and severity.\nDocumenting the moderation logic and thresholds used, including rationale for chosen tool(s).\nMay include\nProviding feedback to users when inputs are blocked.\nLogging flagged prompts for analysis and refinement of filters, while ensuring compliance with privacy obligations.\nFor example, excluding identifying metadata, applying retention limits, and documenting user-facing disclosures or consent mechanisms if required.\nPeriodically evaluating filter performance and adjusting thresholds accordingly.\nFor example, accuracy, latency, false positives/negatives.",
        "url_path": "https://aiuc-1.com/security/implement-real-time-input-filtering"
    },
    {
        "title": "B001. Test adversarial robustness | AIUC-1",
        "section": "Security",
        "heading": "Test adversarial robustness",
        "description": "Implement adversarial testing program to validate system resilience against adversarial inputs and prompt injection attempts in line with adversarial threat taxonomy",
        "keywords": "Adversarial Testing; Red Teaming; Prompt Injection; Jailbreak",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; ISO 42001; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nEstablishing a taxonomy for adversarial risks.\nFor example, referencing and tailoring relevant categories from NIST's AI 100-2e2023 attack classifications (chapters 2.1 and 3.1) such as evasion, poisoning, privacy attacks, and model manipulation, and aligning these to system architecture and use cases.\nConducting comprehensive adversarial testing quarterly and after material system changes.\nFor example, performing structured red-teaming, prompt injection assessments, jailbreaking attempts, adversarial perturbation testing, semantic manipulation, and simulated malicious tool invocations using defined attack trees or scenario templates.\nMaintaining secure testing documentation.\nFor example, recording test cases, methods, outcomes, and system behaviors with restricted access controls, implementing secure storage for sensitive testing materials.\nEstablishing improvement processes based on findings.\nFor example, assigning owners and remediation timelines based on test severity (e.g. critical within 30 days), tracking fixes through risk registers or issue management systems, documenting updates to safeguards and procedures.\nMay include\nAligning adversarial testing with broader security testing programs.\nFor example, integrating AI-specific test cases into penetration testing, sharing threat models across red/blue teams, aligning test cycles with security audit and compliance calendars.",
        "url_path": "https://aiuc-1.com/security/test-adversarial-robustness"
    },
    {
        "title": "B009. Limit output over-exposure | AIUC-1",
        "section": "Security",
        "heading": "Limit output over-exposure",
        "description": "Implement output limitations and obfuscation techniques to reduce information leakage",
        "keywords": "Output Obfuscation; Fidelity Reduction; Information Leakage; Adversarial Use; Response Filtering",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nReducing or limiting the number of results shown in outputs to relevant only to balance security and utility.\nLimiting the output format to reduce exploitability.\nFor example, disabling or redacting structured formats such as JSON, XML, or code snippets where not necessary, especially in externally facing outputs.\nFiltering sensitive information that may reveal internal system behavior.\nFor example, removing or abstracting technical details about model architecture, prompt structure, or tool invocation logic.\nProviding user-facing notices or documentation about output limitations.\nFor example, clearly indicating when results have been truncated, rounded, or suppressed to align with security and privacy safeguards.\nMay include\nLimiting the fidelity of numerical outputs in certain use cases.\nFor example, applying output rounding, threshold bands, or obfuscation techniques to reduce the risk of model inversion or precision-sensitive data disclosure.",
        "url_path": "https://aiuc-1.com/security/limit-output-over-exposure"
    },
    {
        "title": "B008. Protect model deployment environment | AIUC-1",
        "section": "Security",
        "heading": "Protect model deployment environment",
        "description": "Implement security measures for AI model deployment environments including encryption, access controls and authorization",
        "keywords": "Model Environment; Encryption; Access Controls",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; OWASP Top 10",
        "control_activities": "Should include\nImplementing model access protection.\nFor example, restricting access to production AI models based on job function and operational need, implementing MFA for model system access, maintaining user access reviews appropriate to organizational size.\nEstablishing deployment security controls.\nFor example, applying scoped API tokens or signed requests, enforcing rate limits, using TLS for all endpoint traffic, implementing schema validation to protect model APIs from malformed or adversarial input.\nMay include\nSecuring model hosting environments.\nFor example, using up-to-date and minimal container images, scanning for known vulnerabilities in dependencies and base images, and applying infrastructure-level isolation techniques based on risk level (e.g. container namespaces, VM separation, or dedicated GPU access).\nVerifying model integrity before and during deployment.\nFor example, using cryptographic checksums or signed artifacts to detect tampering, scanning model files for malicious payloads (e.g. in Pickle or ONNX formats), and logging model version hashes at deployment time.",
        "url_path": "https://aiuc-1.com/security/protect-model-deployment-environment"
    },
    {
        "title": "B004. Prevent AI endpoint scraping | AIUC-1",
        "section": "Security",
        "heading": "Prevent AI endpoint scraping",
        "description": "Implement safeguards to prevent probing or scraping of external AI endpoints",
        "keywords": "Scraping; Probing; Rate Limiting; Query Quotas; Zero Trust",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nImplementing systems distinguishing between high-volume legitimate usage and adversarial behavior.\nFor example, using behavioral analytics, session history, user role, and customer tier to calibrate thresholds and prevent false positives against known trusted users.\nImplementing rate limiting and query restrictions.\nFor example, establishing per-user query quotas and rate limits to prevent model extraction attempts, configuring automated blocking of excessive query patterns, implementing progressive restrictions for suspicious usage behavior, implementing pseudo limits such as significant price-per-query increases over a set quota.\nConducting simulated external attack testing.\nFor example, performing automated scraping tests, brute force attempts, and reconnaissance activities against AI endpoints, testing rate limiting effectiveness against high-volume query attacks and distributed attacks, documenting test methodologies and scope appropriate to organizational threat profile.\nMaintaining endpoint security through remediation.\nFor example, documenting test results and identified vulnerabilities, implementing protective measures and updating endpoint defenses based on testing outcomes, regularly reviewing and adjusting rate limiting thresholds based on attack patterns.",
        "url_path": "https://aiuc-1.com/security/prevent-ai-endpoint-scraping"
    },
    {
        "title": "B006. Enforce contextual access controls | AIUC-1",
        "section": "Security",
        "heading": "Enforce contextual access controls",
        "description": "Implement safeguards to limit AI agent system access based on context and declared objectives",
        "keywords": "Access Permissions; Agent Permissions",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nConfiguring contextual access controls for AI agents.\nFor example, enforcing task-based tool and data access using declarative policy models (e.g. JSON policy schemas or role-capability matrices), scoping actions based on agent-assigned objectives, session type, or workflow stage.\nImplementing privilege limiting for autonomous behavior.\nFor example, restricting agents from escalating access or acting beyond permitted functions.\nDeploying monitoring and enforcement mechanisms.\nFor example, ensuring AI systems only perform necessary inference and logging deviations from defined operational scope.\nMay include\nDefining automatic restriction triggers.\nFor example, revoking tool access or suppressing outputs when agent context diverges from declared task scope, user role constraints are violated, or anomalous behavior (e.g. lateral tool access or excessive data usage) is detected in real time.\nIntegrating agent access decisions with existing identity and access management (IAM) systems.\nFor example, aligning agent privileges with user roles, enforcing access through API gateways or policy engines, and logging agent actions alongside traditional user activity.",
        "url_path": "https://aiuc-1.com/security/enforce-contextual-access-controls"
    },
    {
        "title": "B003. Limit technical over-disclosure | AIUC-1",
        "section": "Security",
        "heading": "Limit technical over-disclosure",
        "description": "Implement controls to prevent over-disclosure of technical information about AI systems and organizational details that could enable adversarial targeting",
        "keywords": "Public Disclosure; Open-Source; External Threats",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; OWASP Top 10",
        "control_activities": "Should include\nDocumenting limitations on technical information release.\nFor example, limiting public disclosure of model architectures, algorithms, training data details, system configurations, and performance metrics, requiring approval before sharing technical specifications or implementation details.\nControlling organizational information to balance transparency with security.\nFor example, limiting disclosure of AI team details, development timelines, and other information that could reveal technical capabilities, reviewing public communications for sensitive information.\nMay include\nEstablishing approval processes.\nFor example, requiring designated review for public content referencing AI capabilities in e.g. publications, presentations, and marketing materials, and documenting approved disclosures with business justification.",
        "url_path": "https://aiuc-1.com/security/limit-technical-over-disclosure"
    },
    {
        "title": "B007. Enforce AI access privileges | AIUC-1",
        "section": "Security",
        "heading": "Enforce AI access privileges",
        "description": "Establish and maintain access controls and admin privileges for AI systems in line with policy",
        "keywords": "Access Controls; Organizational Policy",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; ISO 42001; OWASP Top 10",
        "control_activities": "Should include\nImplementing system-level access controls tailored to AI systems.\nFor example, using role-based or attribute-based access to restrict access to model configuration, training datasets, tool-calling capabilities, or prompt logs, based on job function and system sensitivity.\nRestricting administrative and configuration privileges to authorized personnel.\nFor example, limiting ability to alter system behavior, tools, or models.\nConducting access reviews and updates at least quarterly.\nFor example, validating access assignments, updating based on policy or role changes,  documenting access changes with AI-specific context (e.g. model access justification, changes to agent capability boundaries, or access to sensitive prompt/response history).",
        "url_path": "https://aiuc-1.com/security/enforce-ai-access-privileges"
    },
    {
        "title": "B002. Detect adversarial input | AIUC-1",
        "section": "Security",
        "heading": "Detect adversarial input",
        "description": "Implement monitoring capabilities to detect and respond to adversarial inputs and prompt injection attempts",
        "keywords": "Monitor; Adversarial; Jailbreak; Prompt Injection",
        "application": "Optional",
        "frequency": "Every 3 months",
        "type": "Detective",
        "crosswalks": "MITRE ATLAS; EU AI Act; ISO 42001; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nEstablishing a taxonomy for adversarial risks.\nFor example, drawing on NIST's AI 100-2e2023 taxonomy for Adversarial Machine Learning.\nEstablishing detection and alerting.\nFor example, implementing monitoring for prompt injection patterns, jailbreak techniques, adversarial input attempts, and exceeding rate limits, configuring alerts and threat notifications for suspicious activities.\nImplementing incident logging and response procedures.\nFor example, logging suspected attacks with timestamps, user/session context, and input content, escalating to designated personnel based on severity thresholds (e.g. immediate escalation for confirmed jailbreaks), documenting response actions in a centralized incident system.\nMaintaining detection effectiveness through quarterly reviews.\nFor example, updating detection rules based on emerging adversarial techniques, analyzing incident patterns and documenting system improvements.\nMay include\nImplementing adversarial input detection prior to AI model processing where feasible.\nFor example, using lightweight pattern-matching, behavioral heuristics, or IP-based filters to flag likely threats before processing, with latency-optimized safeguards or asynchronous review paths where real-time detection is infeasible.\nIntegrating adversarial input detection into existing security operations tooling.\nFor example, forwarding flagged inputs to SIEM platforms, correlating detection with authentication and network logs, enabling SOC teams to triage AI-related security events.",
        "url_path": "https://aiuc-1.com/security/detect-adversarial-input"
    },
    {
        "title": "A005. Prevent cross-customer data exposure | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Prevent cross-customer data exposure",
        "description": "Implement safeguards to prevent cross-customer data exposure when combining customer data from multiple sources for AI model training",
        "keywords": "Cross-Customer Data; Model Training; Data Rights",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nEstablishing explicit consent and disclosure for combined data usage.\nFor example, informing customers when their data will be combined with competitor data, disclosing data anonymization and abstraction policies, providing opt-out mechanisms.\nImplementing customer data isolation controls.\nFor example, enforcing strict logical and physical separation of customer data, applying tenant-specific encryption, validating data flow boundaries in shared infrastructure, establishing technical barriers between customer datasets during training.\nMay include\nImplementing specific privacy-enhancing technologies (PETs) to reduce competitive exposure.\nFor example, applying differential privacy to obfuscate customer contributions, using federated learning to avoid centralizing raw data, generating task-relevant synthetic datasets to simulate real data scenarios.\nImplementing inference-time data isolation to prevent leakage of one customer's data or model-derived insights into responses for other customers.\nFor example, enforcing tenant-aware routing, access control at inference, and prompt context segregation.\nAdapting safeguards to industry-specific competitive risks.\nFor example, applying stricter isolation for customers in the same vertical, avoiding cross-training on data from direct competitors, or honoring industry codes of conduct and regulatory expectations around data co-mingling.",
        "url_path": "https://aiuc-1.com/data-and-privacy/prevent-cross-customer-data-exposure"
    },
    {
        "title": "A004. Protect IP & trade secrets | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Protect IP & trade secrets",
        "description": "Implement safeguards or technical controls to prevent AI systems from leaking company intellectual property or confidential information",
        "keywords": "Intellectual Property; Confidential Information; Data Protections",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; OWASP Top 10",
        "control_activities": "Should include\nDocumenting foundation model provider safeguards which may serve as primary IP protection.\nFor example, reviewing contractual data handling terms, assessing model retention and fine-tuning behaviors, verifying confidentiality commitments, and identifying any limitations or gaps against organizational IP protection criteria.\nEstablishing supplementary data access controls where provider protections are insufficient.\nFor example, limiting AI exposure to proprietary information, implementing role-based access for confidential data, restricting training on internal documents, and applying differentiated controls for open-source versus proprietary assets.\nMay include\nImplementing output monitoring procedures with automated review processes for high-risk scenarios.\nFor example, scanning responses for proprietary code or business information, flagging internal data disclosures.\nMaintaining internal IP incident response and escalation procedures as part of AI failure plan on data breaches.\nFor example, documenting incidents and containment actions, specifying clear escalation timelines and responsible parties based on severity.",
        "url_path": "https://aiuc-1.com/data-and-privacy/protect-ip-trade-secrets"
    },
    {
        "title": "A003. Implement contextual data safeguards | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Implement contextual data safeguards",
        "description": "Implement safeguards to limit AI agent data access to task-relevant information based on user roles and context",
        "keywords": "Data Collection; Data Access",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nConfiguring role-based access controls to limit data access based on user permissions.\nConfiguring data collection limits to reduce privacy exposure.\nFor example, limiting to time-bounded, task-specific, and purpose-limited contextual data, and avoiding persistent or out-of-scope information.\nMay include\nIntegrating with existing identity and access management (IAM) systems to align agent access permissions with organizational policies.\nFor example, SSO, OAuth, OpenID.\nEstablishing dynamic context-based restrictions to adjust access decisions if user role or environment changes during agent session.\nFor example, task-based access controls, contextual capability restrictions, automatic privilege limiting.\nImplementing logging, monitoring and enforcement mechanisms to ensure systems perform only necessary inference functions.\nFor example, logging user identity, data scope, decision rationale, and context to support traceability, audits, and incident response.",
        "url_path": "https://aiuc-1.com/data-and-privacy/implement-contextual-data-safeguards"
    },
    {
        "title": "A007. Prevent IP violations | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Prevent IP violations",
        "description": "Implement safeguards and technical controls to prevent AI outputs from violating copyrights, trademarks, or other third-party intellectual property rights",
        "keywords": "Intellectual Property; Copyright Protection",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nDocumenting and evaluating foundation model provider IP protections which may serve as primary infringement safeguards.\nFor example, reviewing copyright and trademark handling, assessing indemnification coverage, and identifying known limitations, exclusions, or risk thresholds relevant to organizational context.\nEstablishing supplementary content filtering mechanisms where provider protections have gaps or limitations.\nFor example, detecting copyrighted material in outputs, implementing trademark screening.\nMay include\nImplementing user guidance and guardrails to reduce IP risk.\nFor example, providing usage policies that explain prohibited content types, embedding warnings or UI notices for high-risk prompts, restricting output generation in known infringement domains.\nMaintaining third-party IP incident response procedures.\nFor example, identifying potential infringement, documenting incidents and remediation actions, coordinating with provider protections.\nImplementing restrictions in AI acceptable use policy.",
        "url_path": "https://aiuc-1.com/data-and-privacy/prevent-ip-violations"
    },
    {
        "title": "A002. Define output rights | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Define output rights",
        "description": "Establish AI output ownership, usage, opt-out and deletion policies to customers and communicate these policies",
        "keywords": "Data Ownership; Usage; Deletion; Consent; Opt-Out",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "ISO 42001",
        "control_activities": "Should include\nDefining output ownership rights with clear distinctions between customer inputs and AI outputs.\nFor example, specifying customer versus organization ownership of AI-generated content, usage permissions for different output types.\nDisclosing consent and opt-out procedures.\nFor example, documenting consent collection, user-accessible opt-out controls, opt-out limitations, and preference workflows.\nEstablishing output usage policies communicated through accessible terms of service.\nFor example, permitted uses of AI-generated content, restrictions on redistribution or commercial use, intellectual property considerations, policies on derivative and transformative use.",
        "url_path": "https://aiuc-1.com/data-and-privacy/define-output-rights"
    },
    {
        "title": "A006. Prevent PII leakage | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Prevent PII leakage",
        "description": "Establish safeguards to prevent personal data leakage through AI outputs",
        "keywords": "Personal Data Leakage",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nEstablishing data segregation controls.\nFor example, isolating user sessions, implementing user-specific boundaries, preventing reuse of prompts or outputs containing personal identifiers, maintaining dataset isolation.\nEstablishing safeguards to prevent personal data leakage between users.\nFor example, isolating user sessions, applying user-specific output boundaries, and preventing reuse of prompts or outputs containing personal identifiers.\nDocumenting protection procedures and incident management.\nFor example, identifying PII, defining output handling policies, maintaining leakage incident records and remediation actions.\nMay include\nImplementing output monitoring.\nFor example, scanning outputs for cross-customer data leakage, validating data source attribution.\nImplementing automated detection and redaction of personal data in AI outputs.\nFor example, using named entity recognition (NER) or data classification tools to scan and remove PII before output is delivered to end users.\nIntegrating with existing data loss prevention (DLP) systems to monitor and block outputs containing personal data in violation of policy.",
        "url_path": "https://aiuc-1.com/data-and-privacy/prevent-pii-leakage"
    },
    {
        "title": "A001. Establish data use policy | AIUC-1",
        "section": "Data & Privacy",
        "heading": "Establish data use policy",
        "description": "Establish and communicate AI input data policies covering how customer data is used for model training, inference processing, data retention periods, and customer data rights",
        "keywords": "Data Retention; Model Training Data; Opt-Out",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nDefining data usage policies.\nFor example, opt-in mechanisms, disclosure requirements, boundaries between training and post-deployment data usage.\nImplementing data retention and deletion procedures.\nFor example, retention periods for training data, inference logs, and customer inputs, plus technical approaches for data removal such as selective unlearning or system retirement when data cannot be effectively separated from models.\nDocumenting how data retention periods are calculated and justified.\nMay include\nDocumenting data subject rights processes.\nFor example, handling customer requests for data access, portability, and deletion in AI contexts, plus maintaining records of which datasets were used to train specific models.",
        "url_path": "https://aiuc-1.com/data-and-privacy/establish-data-use-policy"
    },
    {
        "title": "E006. Conduct vendor due diligence | AIUC-1",
        "section": "Accountability",
        "heading": "Conduct vendor due diligence",
        "description": "Establish AI vendor due diligence processes for foundation and upstream model providers covering data handling, PII controls, security and compliance",
        "keywords": "Vendor Due Diligence; Open-Source; Foundation Models; Upstream Models",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nDefining assessment criteria for foundational or upstream AI models.\nFor example, data handling practices, PII controls, security measures, compliance status, open-source.\nConducting documented assessments.\nFor example, scoring results, verification activities such as certifications reviewed and references contacted, and approval decisions. Can follow a RACI structure.\nMaintaining assessment records with sufficient detail for audit purposes and retaining due diligence evidence before vendor approval.",
        "url_path": "https://aiuc-1.com/accountability/conduct-vendor-due-diligence"
    },
    {
        "title": "E010. Establish AI acceptable use policy | AIUC-1",
        "section": "Accountability",
        "heading": "Establish AI acceptable use policy",
        "description": "Establish and implement an AI acceptable use policy",
        "keywords": "Acceptable Use; Breach",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "ISO 42001; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nDefining prohibited AI usage.\nFor example, jailbreak attempts, malicious prompt injection, unauthorized data extraction, generation of harmful content, and misuse of customer data ideally with specific examples.\nImplementing detection and monitoring tools.\nFor example, prompt analysis, output filtering, usage pattern anomalies, and suspicious access attempts.\nMay include\nReal-time monitoring, blocking, or alerting capabilities.\nMaintaining logging and tracking systems.\nFor example, incident creation, violation tracking with case assignment and resolution documentation.\nEstablishing incident response procedures.\nFor example,  defining violation severity levels with corresponding response priorities, implementing escalation procedures with defined timeframes for user account restrictions and security notifications, documenting containment actions including system access modifications.\nConducting regular effectiveness reviews.\nFor example, quarterly analysis of violation trends, tool performance assessment, policy updates based on emerging threats, and user training adjustments.",
        "url_path": "https://aiuc-1.com/accountability/establish-ai-acceptable-use-policy"
    },
    {
        "title": "E001. AI failure plan for security breaches | AIUC-1",
        "section": "Accountability",
        "heading": "AI failure plan for security breaches",
        "description": "Document AI failure plan for AI privacy and security breaches assigning accountable owners and establishing notification and remediation with third-party support as needed (e.g. legal, PR, insurers)",
        "keywords": "Incident Response; Security; Privacy; Regulatory Deadlines",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nAssigning a breach response lead from existing staff.\nFor example, IT manager, security officer, or designated executive with authority to engage external counsel and specialists as needed.\nDefining breach notification procedures.\nFor example, customer communications, regulatory reporting requirements, and vendor notifications based on applicable privacy laws.\nImplementing security remediation measures.\nFor example, system freeze capabilities, vulnerability fixes, access control updates, and coordination with external security consultants when internal expertise is insufficient.\nEstablishing evidence collection requirements with guidance on preserving evidence for potential legal review.\nFor example, system logs, user activity records, and basic documentation.",
        "url_path": "https://aiuc-1.com/accountability/ai-failure-plan-for-security-breaches"
    },
    {
        "title": "E014. Share transparency reports | AIUC-1",
        "section": "Accountability",
        "heading": "Share transparency reports",
        "description": "Establish policies for sharing transparency reports with relevant stakeholders including regulators and customers",
        "keywords": "Transparency",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nDefining report scope and recipient categories with clear criteria for when reports must be shared.\nFor example, regulators, customers, and other stakeholders.\nExcluding or sanitizing technical documentation and other sensitive information that could be used for adversarial attacks.\nMay include\nImplementing secure delivery methods with appropriate authentication and access control.\nFor example,  dataroom access, encrypted transmission, controlled access portals.\nDocumenting sharing procedures including approval workflows, version control, and audit trails for transparency.",
        "url_path": "https://aiuc-1.com/accountability/share-transparency-reports"
    },
    {
        "title": "E017. Document system transparency policy | AIUC-1",
        "section": "Accountability",
        "heading": "Document system transparency policy",
        "description": "Establish a system transparency policy and maintain a repository of model cards, datasheets, and interpretability reports for major systems",
        "keywords": "Transparency; System Cards",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nEstablishing a transparency policy defining requirements for documentation of major AI systems.\nFor example, model capabilities, limitations, and intended use cases.\nMaintaining a centralized repository of system documentation with appropriate access controls for internal stakeholders.\nFor example, model cards, datasheets, and interpretability reports.\nImplementing updates to documentation when systems are modified or new information becomes available about model performance or risks.",
        "url_path": "https://aiuc-1.com/accountability/document-system-transparency-policy"
    },
    {
        "title": "E008. Review internal processes | AIUC-1",
        "section": "Accountability",
        "heading": "Review internal processes",
        "description": "Establish regular internal reviews of key processes and document review records and approvals",
        "keywords": "Internal Reviews; Documentation",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nReviewing decision processes every quarter including AI system changes, foundational model selection, security assessment.\nMaintaining a centralized repository of decision records and internal review of these record.\nFor example, supporting evidence reviewed, remediation plans.\nDocumenting and tracking remediation of any risks identified.\nMay include\nCollecting and implementing external feedback on AI systems.\nFor example, system risks, new threat patterns, new mitigation strategies.",
        "url_path": "https://aiuc-1.com/accountability/review-internal-processes"
    },
    {
        "title": "E009. Monitor 3rd-party access | AIUC-1",
        "section": "Accountability",
        "heading": "Monitor 3rd-party access",
        "description": "Implement systems to monitor third party access",
        "keywords": "Access; Logins",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nDefining third-party interaction scope with logging of access attempts and activities.\nFor example, API connections, user access sessions, data exchanges, and service integrations.\nCapturing access metadata.\nFor example, user identification, authentication timestamps, accessed resources, session duration, origin IP addresses, and resource usage patterns.",
        "url_path": "https://aiuc-1.com/accountability/monitor-3rd-party-access"
    },
    {
        "title": "E004. Assign accountability | AIUC-1",
        "section": "Accountability",
        "heading": "Assign accountability",
        "description": "Document which AI system changes across the development & deployment lifecycle require formal review or approval, assign a lead accountable for each, and document their approval with supporting evidence",
        "keywords": "Decision Owners; Deployment",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nDefining AI system changes requiring approval including model selection, material changes to the meta prompt, adding / removing guardrails, changes to end-user workflow, other changes that drive material.\nFor example, +/-10% performance on evals.\nAssigning an accountable lead as approver for each of these changes.\nCan follow a RACI structure to formalize roles of those consulted and informed.\nDocumenting approval in a repository retained for 365+ days, including what data was reviewed as part of the decision.\nFor example, results of evaluations, internal or external red-teaming, customer feedback.\nMay include\nImplementing code signing and verification processes for AI models, libraries, and deployment artifacts to ensure only digitally signed components are approved for production use.",
        "url_path": "https://aiuc-1.com/accountability/assign-accountability"
    },
    {
        "title": "E007. Document system change approvals | AIUC-1",
        "section": "Accountability",
        "heading": "Document system change approvals",
        "description": "Define approval processes for material changes to AI systems (e.g. model versions, access controls, data sources) requiring formal review and sign-off",
        "keywords": "Approvals; Workflows",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Detective",
        "crosswalks": "EU AI Act; NIST AI RMF",
        "control_activities": "Should include\nDefining changes requiring formal review and approval with clear approval thresholds such as changes affecting system performance or security.\nFor example, model updates, prompt modifications, adding/removing guardrails, and changes to user-facing functionality.\nDocumenting the approval workflow with sufficient detail for review purposes.\nFor example, who approves what types of changes and maintaining records of approvals.",
        "url_path": "https://aiuc-1.com/accountability/document-system-change-approvals"
    },
    {
        "title": "E005. Assess cloud vs on-prem processing | AIUC-1",
        "section": "Accountability",
        "heading": "Assess cloud vs on-prem processing",
        "description": "Establish criteria for selecting cloud provider, and circumstances for on-premises processing considering data sensitivity, regulatory requirements, security controls, and operational needs",
        "keywords": "Deployment; Cloud Security; On-Premise Security; Data Residency",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nConducting deployment risk assessments.\nFor example, evaluating data sensitivity, regulatory compliance requirements, IP protection needs, and security controls for cloud vs. on-premises AI processing.\nDocumenting decision criteria and rationale.\nFor example, establishing clear selection factors, maintaining records of deployment choices with business justification.\nImplementing deployment-appropriate security controls.\nFor example, configuring cloud-specific protections or on-premises security measures based on selected deployment model.\nMay include\nImplementing hybrid deployment strategies.\nFor example, using on-premises for sensitive data, cloud for less sensitive workloads, with secure data flow controls.\nEstablishing cloud vendor management procedures.\nFor example, conducting provider due diligence, implementing contractual protections for data sovereignty and IP.\nReviewing deployment decisions when requirements change.\nFor example, reassessing choices when data sensitivity, regulations, or threat landscape evolves.",
        "url_path": "https://aiuc-1.com/accountability/assess-cloud-vs-on-prem-processing"
    },
    {
        "title": "E002. AI failure plan for harmful outputs | AIUC-1",
        "section": "Accountability",
        "heading": "AI failure plan for harmful outputs",
        "description": "Document AI failure plan for harmful AI outputs that cause significant customer harm assigning accountable owners and establishing remediation with third-party support as needed (e.g. legal, PR, insurers)",
        "keywords": "Incident Response; Emergency Response; Harmful Outputs; Hallucinations; Vendors",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nImplementing customer communication protocols.\nFor example, disclosure procedures, explanation of corrective actions, and follow-up commitments with executive approval for significant incidents.\nEstablishing immediate mitigation steps with designated staff responsibilities.\nFor example, system freeze capabilities, output suppression, customer notification, and system adjustments.\nMay include\nDefining harmful output categories with reference to risk taxonomy.\nFor example, discriminatory content, offensive material, inappropriate recommendations, ideally with concrete examples.\nCoordinating external support engagement.\nFor example,  legal counsel consultation, PR support, and insurance claim procedures.",
        "url_path": "https://aiuc-1.com/accountability/ai-failure-plan-for-harmful-outputs"
    },
    {
        "title": "E012. Document regulatory compliance | AIUC-1",
        "section": "Accountability",
        "heading": "Document regulatory compliance",
        "description": "Document applicable AI laws and standards, required data protections, and strategies for compliance",
        "keywords": "Regulatory; EU; NY; NIST; ISO; GDPR",
        "application": "Mandatory",
        "frequency": "Every 6 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nIdentifying relevant regulations.\nFor example, data protection laws. For example, GDPR, CCPA, sector-specific requirements, emerging AI standards. For example, EU AI Act.\nDocumenting compliance procedures and strategies appropriate for company size and operations.\nReviewing the repository every 6 months and when additional requirements may be triggered.\nFor example, regulations change or business operations expand into new jurisdictions.",
        "url_path": "https://aiuc-1.com/accountability/document-regulatory-compliance"
    },
    {
        "title": "E003. AI failure plan for hallucinations | AIUC-1",
        "section": "Accountability",
        "heading": "AI failure plan for hallucinations",
        "description": "Document AI failure plan for hallucinated AI outputs that cause substantial customer financial loss assigning accountable owners and establishing remediation with third-party support as needed (e.g. legal, PR, insurers)",
        "keywords": "Hallucinations; Incident Response; Customer Loss",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nEstablishing compensation assessment procedures.\nFor example, loss evaluation methods, settlement approaches, and payment authorization levels with appropriate approval requirements.\nImplementing remediation measures.\nFor example, system freeze capabilities, model adjustments, output validation improvements, customer notification, and enhanced monitoring.\nMay include\nDefining hallucination incident types.\nFor example,  factual errors or incorrect recommendations relevant to company context and customer base.\nCoordinating potential external support.\nFor example, legal consultation for significant claims, financial review when needed, and insurance coverage activation.",
        "url_path": "https://aiuc-1.com/accountability/ai-failure-plan-for-hallucinations"
    },
    {
        "title": "E013. Implement quality management system | AIUC-1",
        "section": "Accountability",
        "heading": "Implement quality management system",
        "description": "Establish a quality management system for high-risk AI systems proportionate to the size of the organization",
        "keywords": "EU; Quality management; Regulatory",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; NIST AI RMF; ISO 42001",
        "control_activities": "Should include\nDocumenting strategy for compliance with conformity assessment procedures.\nDocumenting techniques, procedures and systematic actions to be used for the design, design control and design verification of the AI system.\nDocumenting techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the AI system.\nDocumenting the handling of communication with national competent authorities, other relevant authorities, including those providing or supporting the access to data, notified bodies, other operators, customers or other interested parties.\nDocumenting resource management, including security-of-supply related measures.\nAssigning and documenting accountability in the organisation for each of the aspects in the quality management system.\nMay include\nCollecting comprehensive documentation for EU AI Act Article 17 requirements for quality management systems.\nFor example, strategy for regulatory compliance, examination, test and validation procedures, technical specifications and fulfillment, data management procedures, risk management, post-market monitoring, incident reporting, and record-keeping.",
        "url_path": "https://aiuc-1.com/accountability/implement-quality-management-system"
    },
    {
        "title": "E016. Implement AI disclosure mechanisms | AIUC-1",
        "section": "Accountability",
        "heading": "Implement AI disclosure mechanisms",
        "description": "Implement clear disclosure mechanisms to inform users when they are interacting with AI systems rather than human operators",
        "keywords": "Labelling; Transparency",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nImplementing clear AI interaction disclosure at the beginning of communications, notifying users they are interacting with artificial intelligence, not humans.\nEnsuring disclosures are conspicuous and easily understood.\nFor example, using prominent placement and plain language appropriate for the communication medium.\nMaintaining disclosure visibility throughout extended interactions.\nFor example, providing ongoing indication of AI involvement in conversations or sessions.\nLabelling AI generated audio, image and video in a machine-readable format and detectable as artificially generated or manipulated.\nTechnical solutions must be effective, interoperable, robust and reliable as far as this is technically feasible.\nInforming users if they are exposed to emotion recognition or biometric categorisation systems.\nMay include\nImplementing adaptive disclosure methods for different interaction types.\nFor example, visual indicators for text, audio notifications for voice communications.\nEstablishing reactive disclosure capabilities when users ask if they are interacting with AI.",
        "url_path": "https://aiuc-1.com/accountability/implement-ai-disclosure-mechanisms"
    },
    {
        "title": "E015. Log model activity | AIUC-1",
        "section": "Accountability",
        "heading": "Log model activity",
        "description": "Maintain logs of AI system processes, actions, and model outputs where permitted to support incident investigation, auditing, and explanation of AI system behavior",
        "keywords": "Explainability; Logs",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Detective",
        "crosswalks": "MITRE ATLAS; EU AI Act; ISO 42001; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nCapturing system activity details.\nFor example, input parameters, processing steps, model outputs, and user interactions.\nImplementing log storage with appropriate retention periods and access controls to support auditing and incident response.",
        "url_path": "https://aiuc-1.com/accountability/log-model-activity"
    },
    {
        "title": "E011. Record processing locations | AIUC-1",
        "section": "Accountability",
        "heading": "Record processing locations",
        "description": "Document AI data processing locations",
        "keywords": "Data Processing; Storage Location; Data Protections",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nMaintaining AI infrastructure location documentation.\nFor example, geographic locations of foundation model processing locations and inference endpoint regions, documenting third-party AI service provider data handling locations.\nReviewing and updating documentation regularly.\nMay include\nImplementing transfer compliance procedures.\nFor example, assessing data transfer requirements for AI training data and inference processing, maintaining approved transfer mechanisms for foundation model providers and AI infrastructure, mitigating transfer risk for cross-border AI model training.",
        "url_path": "https://aiuc-1.com/accountability/record-processing-locations"
    },
    {
        "title": "F002. Prevent catastrophic misuse | AIUC-1",
        "section": "Society",
        "heading": "Prevent catastrophic misuse",
        "description": "Implement or document guardrails to prevent AI-enabled catastrophic system misuse (chemical / bio / radio / nuclear)",
        "keywords": "CBRN; Chemical; Bioweapon; Radioactive; Nuclear",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Detective",
        "crosswalks": "",
        "control_activities": "Should include\nResults of testing from foundation model developer on CBRN capabilities and mitigations.\nAttestation that the mitigations have not been removed.\nMay include\nRelevant evaluations.\nFor example, Center for AI Safety's Weapons of Mass Destruction proxy benchmark.\nEstablishing catastrophic misuse monitoring.\nFor example, monitoring AI system interactions for patterns indicating weapons development or mass harm intent, implementing real-time alerting for detected catastrophic misuse attempts, documenting suspicious queries and system responses.",
        "url_path": "https://aiuc-1.com/society/prevent-catastrophic-misuse"
    },
    {
        "title": "F001. Prevent AI cyber misuse | AIUC-1",
        "section": "Society",
        "heading": "Prevent AI cyber misuse",
        "description": "Implement or document guardrails to prevent AI-enabled misuse for cyber attacks and exploitation",
        "keywords": "Cyber Attacks",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF",
        "control_activities": "Should include\nResults of testing from foundation model developer on offensive cyber capabilities and mitigations.\nAttestation the mitigations have not been removed.\nMay include\nImplementing malicious use detection and blocking.\nFor example, deploying available content filtering to detect requests for malicious code generation, attack planning, and vulnerability exploitation guidance, configuring automated blocking of cyber attack assistance requests, maintaining databases of prohibited use patterns.\nEstablishing usage monitoring and threat intelligence.\nFor example, monitoring AI system usage for exploitation attempts and suspicious patterns, maintaining updated threat intelligence on AI misuse techniques, implementing alerting for detected malicious use attempts.",
        "url_path": "https://aiuc-1.com/society/prevent-ai-cyber-misuse"
    },
    {
        "title": "C008. Monitor AI risk categories | AIUC-1",
        "section": "Safety",
        "heading": "Monitor AI risk categories",
        "description": "Implement monitoring of AI systems across risk categories",
        "keywords": "Monitoring; High-Risk Outputs",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Detective",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nImplementing proactive detection.\nFor example, defining potential scenarios that could generate harmful outputs under normal or adversarial use, documenting risk scenarios to guide test planning and operational safeguards aligned with risk taxonomy, deploying automated detection tools (e.g. classifiers, heuristics, anomaly detectors).\nEstablishing ongoing monitoring.\nFor example, conducting regular evaluations prioritized by risk severity, using methods such as output sampling, behavior tracing, and prompt-response logging.\nMaintaining documentation.\nFor example, recording identified scenarios with clear examples, conditions, and mitigation approaches, updating risk taxonomy based on monitoring findings and incidents.\nMay include\nIntegrating AI output monitoring with existing security tools.\nFor example, forwarding alerts and flagged outputs to SIEM platforms, applying standard logging formats (e.g. JSON, syslog) to support automated threat detection workflows.",
        "url_path": "https://aiuc-1.com/safety/monitor-ai-risk-categories"
    },
    {
        "title": "C011. 3rd-party testing for out-of-scope outputs | AIUC-1",
        "section": "Safety",
        "heading": "3rd-party testing for out-of-scope outputs",
        "description": "Appoint expert third-parties to evaluate system robustness to out-of-scope outputs at least every 3 months (e.g. political discussion, healthcare advice)",
        "keywords": "Out-of-Scope; Political Discussion; Third-Party Testing",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF",
        "control_activities": "Should include\nAppointing qualified third-party assessors.\nFor example, selecting assessors with relevant technical capabilities for identified risk areas, maintaining records of assessor qualifications and independence.\nConducting regular testing.\nFor example, performing assessments of out-of-scope outputs at least every quarter, defining testing scope and methodologies based on risk taxonomy.\nMaintaining documentation.\nFor example, recording third-party qualifications, testing scope, results, and remediation actions taken, tracking follow-up activities and resolution timelines.",
        "url_path": "https://aiuc-1.com/safety/3rd-party-testing-for-out-of-scope-outputs"
    },
    {
        "title": "C006. Prevent output vulnerabilities | AIUC-1",
        "section": "Safety",
        "heading": "Prevent output vulnerabilities",
        "description": "Implement safeguards to prevent security vulnerabilities in outputs from impacting users",
        "keywords": "Harmful Outputs; Code Injection; Data Exfiltration",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; OWASP Top 10",
        "control_activities": "Should include\nEstablishing output sanitization and validation procedures before presenting content to users.\nFor example, stripping or encoding HTML, JavaScript, shell syntax, and iframe content, blocking or rewriting unsafe URLs, validating structured output schemas (e.g. JSON/YAML/XML) against whitelists, enforcing safe rendering modes (e.g. text-only, content-security-policy (CSP) headers).\nImplementing safety-specific labeling and handling protocols.\nFor example, clearly marking untrusted, distinguishing untrusted third-party data, applying appropriate security controls based on content source and risk level.\nMaintaining detection and monitoring capabilities.\nFor example, logging sanitization activities, implementing alerting for suspicious content patterns.\nMay include\nDetecting advanced output-based attack patterns.\nFor example, identifying prompt injection chains, model-output subversion (e.g. jailbreak tokens), payloads targeting downstream applications (e.g. command-line instructions, SQL queries), or obfuscated exploits designed to bypass basic filters.",
        "url_path": "https://aiuc-1.com/safety/prevent-output-vulnerabilities"
    },
    {
        "title": "C010. 3rd-party testing for harmful outputs | AIUC-1",
        "section": "Safety",
        "heading": "3rd-party testing for harmful outputs",
        "description": "Appoint expert third-parties to evaluate system robustness to harmful outputs including distressed outputs, angry responses, high-risk advice, offensive content, bias, and deception at least every 3 months",
        "keywords": "Harmful Outputs; Distressed; Angry; Advice; Offensive; Bias; Risk Severity; Toxigen; Third-Party Testing",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; EU AI Act",
        "control_activities": "Should include\nAppointing qualified third-party assessors.\nFor example, selecting assessors with relevant technical capabilities for identified risk areas, maintaining records of assessor qualifications and independence.\nConducting regular testing.\nFor example, performing assessments of harmful outputs at least every quarter, defining testing scope and methodologies based on risk classifications and industry benchmarks like ToxiGen, coordinating with internal security and testing teams.\nMaintaining documentation.\nFor example, recording third-party qualifications, testing scope, results, and remediation actions taken, tracking follow-up activities and resolution timelines.",
        "url_path": "https://aiuc-1.com/safety/3rd-party-testing-for-harmful-outputs"
    },
    {
        "title": "C001. Define AI risk taxonomy | AIUC-1",
        "section": "Safety",
        "heading": "Define AI risk taxonomy",
        "description": "Establish a risk taxonomy that categorizes risks within harmful, out-of-scope, and hallucinated outputs, tool calls, and other risks based on application-specific usage",
        "keywords": "Risk Taxonomy; Severity Rating",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nDefining risk categories with severity levels and examples based on industry and deployment context.\nFor example, classifying harmful outputs such as distressed outputs, angry responses, high-risk advice, offensive content, bias, and deception, identifying other high-risk use cases such as safety-critical instructions, legal recommendations, financial advice.\nAligning risk taxonomy with external frameworks and standards.\nFor example, NIST AI RMF functions, EU AI Act article 9, ISO42001 controls.\nEstablishing severity grading appropriate to organizational context and risk tolerance.\nFor example, implementing consistent scoring methodology across risk categories, defining thresholds for flagging and human review.\nMaintaining taxonomy currency with documented change management.\nFor example, reviewing and updating risk categories quarterly or when new threat patterns emerge, adjusting risk thresholds, incorporating lessons from incident response and industry benchmarks.\nMay include\nIdentifying additional risk categories that are considered harmful given nature of operations.\nFor example, hallucinations, out-of-scope content.",
        "url_path": "https://aiuc-1.com/safety/define-ai-risk-taxonomy"
    },
    {
        "title": "C003. Prevent harmful outputs | AIUC-1",
        "section": "Safety",
        "heading": "Prevent harmful outputs",
        "description": "Implement safeguards or technical controls to prevent harmful outputs including distressed outputs, angry responses, high-risk advice, offensive content, bias, and deception",
        "keywords": "Harmful Outputs; Distressed; Angry; Advice; Offensive; Bias",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nImplementing content filtering for harmful output types.\nFor example, detecting and blocking distressed responses, angry language, offensive content, biased statements, and deceptive information.\nEstablishing safety guardrails for advice generation.\nFor example, restricting high-risk recommendations in sensitive domains, requiring disclaimers for guidance.\nMaintaining bias detection and mitigation controls.\nFor example, monitoring for discriminatory patterns, implementing fairness checks in outputs.\nMay include\nEvaluating harm mitigation controls using performance metrics.\nFor example, tracking false positives (overblocking safe content) and false negatives (missed harmful content), measuring coverage of flagged scenarios, and benchmarking against known harm datasets like ToxiGen.\nEstablishing review and appeal mechanisms.\nFor example, allowing flagged outputs to be escalated for manual review, recording override decisions with justification, incorporating feedback into harm detection refinement.",
        "url_path": "https://aiuc-1.com/safety/prevent-harmful-outputs"
    },
    {
        "title": "C007. Flag high risk recommendations | AIUC-1",
        "section": "Safety",
        "heading": "Flag high risk recommendations",
        "description": "Implement an alerting system that flags high-risk recommendations for human review",
        "keywords": "Human Review; Escalation",
        "application": "Optional",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; NIST AI RMF",
        "control_activities": "Should include\nDefining high-risk recommendation criteria drawing on risk taxonomy.\nFor example, financial advice exceeding company thresholds, medical or health-related guidance, legal recommendations, safety-critical instructions, and content that could cause reputational harm.\nImplementing automated detection using keyword filtering, confidence scoring, or rule-based assessment with adjustable sensitivity settings.\nEstablishing human review workflows.\nFor example, designated reviewers from available staff, escalation procedures for complex cases, queue management for pending reviews with response time tracking against SLA, documentation of review decisions.",
        "url_path": "https://aiuc-1.com/safety/flag-high-risk-recommendations"
    },
    {
        "title": "C005. Prevent other high risk outputs | AIUC-1",
        "section": "Safety",
        "heading": "Prevent other high risk outputs",
        "description": "Implement safeguards or technical controls to prevent additional high-risk outputs as defined in risk taxonomy",
        "keywords": "High-Risk Outputs; Risk Taxonomy; Technical Controls",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nImplementing detection and blocking mechanisms aligned with organizational risk taxonomy.\nFor example, deploying filtering based on defined risk categories and severity thresholds.\nMaintaining risk-based response controls.\nFor example, flagging and blocking mechanisms, logging for monitoring purposes.\nMay include\nEstablishing escalation procedures for flagged high-risk content.\nFor example, human review workflows, approval requirements for edge cases, planning reviewer capacity based on expected flagging volume and response time objectives.\nImplementing automated real-time response mechanisms.\nFor example, triggering dynamic warnings, blocking or modifying model responses based on severity thresholds, routing flagged interactions for further processing or audit without user delay.",
        "url_path": "https://aiuc-1.com/safety/prevent-other-high-risk-outputs"
    },
    {
        "title": "C002. Conduct pre-deployment testing | AIUC-1",
        "section": "Safety",
        "heading": "Conduct pre-deployment testing",
        "description": "Conduct internal testing of AI systems prior to deployment across risk categories (including high-risk, harmful, hallucinated, and out-of-scope outputs and tool calls) for system changes requiring formal review or approval",
        "keywords": "Internal Testing; Pre-Deployment Testing",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nConducting pre-deployment testing with documented results and identified issues.\nFor example, structured hallucination testing, adversarial prompting, safety unit tests, and scenario-based walkthroughs.\nCompleting risk assessments of identified issues before system deployment.\nFor example, potential impact analysis, mitigation strategies, and residual risk evaluation.\nObtaining approval sign-offs from designated accountable leads with documented rationale for approval decisions and maintained records for review purposes.\nMay include\nIntegrating AI system testing into established software development lifecycle (SDLC) gates.\nFor example, requiring risk evaluation and sign-off at staging or pre-production milestones, aligning with CI/CD or MLOps pipelines, and documenting test artifacts in shared repositories.\nImplementing pre-deployment vulnerability scanning of AI artifacts and dependencies.\nFor example, scanning model files (e.g. pickle, ONNX) for malicious code or unsafe deserialization, checking runtime behavior for unbounded tool execution or insecure API access, validating ML libraries and infrastructure for known CVEs, and analyzing downstream outputs for unsafe content or behaviors.",
        "url_path": "https://aiuc-1.com/safety/conduct-pre-deployment-testing"
    },
    {
        "title": "C009. Collect real-time feedback | AIUC-1",
        "section": "Safety",
        "heading": "Collect real-time feedback",
        "description": "Implement mechanisms to enable real-time user feedback collection and intervention mechanisms",
        "keywords": "Feedback; Intervention; User Control; Transparency",
        "application": "Optional",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "EU AI Act; ISO 42001; NIST AI RMF",
        "control_activities": "Should include\nEstablishing on-screen communication systems.\nFor example, implementing real-time display of system status, intervention notices, disclaimers, and risk alerts during interactions, ensuring messages are context-sensitive and clearly visible.\nEnabling user intervention capabilities.\nFor example, providing mechanisms for users to pause, stop, or redirect system behavior, implementing feedback collection tools for users to report issues or concerns, ensuring technical controls persist across devices and interaction contexts.\nEnsuring accessibility of feedback and intervention mechanisms.\nFor example, adhering to WCAG 2.1 standards for color contrast, screen reader compatibility, keyboard navigation, and clear messaging for users with disabilities.\nMaintaining mechanism effectiveness through quarterly reviews.\nFor example, evaluating user feedback and intervention patterns, adapting communication methods based on user needs and emerging risk considerations.\nMay include\nAnalyzing collected feedback using structured methodologies.\nFor example, categorizing by risk domain, prioritizing based on frequency and severity,  routing high-impact or repeat issues into product backlog or compliance workflows.",
        "url_path": "https://aiuc-1.com/safety/collect-real-time-feedback"
    },
    {
        "title": "C012. 3rd-party testing for other risk | AIUC-1",
        "section": "Safety",
        "heading": "3rd-party testing for other risk",
        "description": "Appoint expert third-parties to evaluate system robustness to additional high-risk outputs as defined in risk taxonomy at least every 3 months",
        "keywords": "High-Risk Outputs; Risk Taxonomy; Third-Party Testing",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF",
        "control_activities": "Should include\nAppointing qualified third-party assessors.\nFor example, selecting assessors with relevant technical capabilities for identified risk areas, maintaining records of assessor qualifications and independence.\nConducting regular testing.\nFor example, performing assessments of high-risk areas at least every quarter, defining testing scope and methodologies based on risk taxonomy.\nMaintaining documentation.\nFor example, recording third-party qualifications, testing scope, results, and remediation actions taken, tracking follow-up activities and resolution timelines.",
        "url_path": "https://aiuc-1.com/safety/3rd-party-testing-for-other-risk"
    },
    {
        "title": "C004. Prevent out-of-scope outputs | AIUC-1",
        "section": "Safety",
        "heading": "Prevent out-of-scope outputs",
        "description": "Implement safeguards or technical controls to prevent out-of-scope outputs (e.g. political discussion, healthcare advice)",
        "keywords": "Out-of-Scope; Political Discussion; Technical Controls",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nImplementing topic boundary enforcement.\nFor example, detecting and redirecting conversations outside intended use cases as defined in AI acceptable use policy, blocking prohibited discussion areas such as political topics or out-of-scope advice.\nEstablishing scope violation response procedures.\nFor example, automated redirection messages, escalation for persistent attempts.\nMaintaining scope monitoring and adjustment capabilities.\nFor example, tracking boundary violations, updating restrictions based on emerging issues.\nMay include\nImplementing user education on system scope and limitations.\nFor example, displaying onboarding tooltips, publishing usage guidelines or FAQs, embedding in-context hints to clarify intended capabilities, highlighting unsupported domains to reduce misuse.",
        "url_path": "https://aiuc-1.com/safety/prevent-out-of-scope-outputs"
    },
    {
        "title": "D001. Prevent hallucinated outputs | AIUC-1",
        "section": "Reliability",
        "heading": "Prevent hallucinated outputs",
        "description": "Implement safeguards or technical controls to prevent hallucinated outputs",
        "keywords": "Hallucinations; Technical Controls",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10; EU AI Act",
        "control_activities": "Should include\nImplementing factual accuracy controls.\nFor example, deploying available fact-checking mechanisms, flagging uncertain or low-confidence responses.\nEstablishing information source validation.\nFor example, requiring citations for factual claims, implementing source reliability checks.\nMaintaining uncertainty communication.\nFor example, displaying confidence levels, providing appropriate disclaimers for generated information.",
        "url_path": "https://aiuc-1.com/reliability/prevent-hallucinated-outputs"
    },
    {
        "title": "D003. Restrict unsafe tool calls | AIUC-1",
        "section": "Reliability",
        "heading": "Restrict unsafe tool calls",
        "description": "Implement safeguards or technical controls to prevent tool calls in AI systems from executing unauthorized actions, accessing restricted information, or making decisions beyond their intended scope",
        "keywords": "Tool Calls; Tool Selection; Technical Controls",
        "application": "Mandatory",
        "frequency": "Every 12 months",
        "type": "Preventative",
        "crosswalks": "MITRE ATLAS; EU AI Act; NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nImplementing function call validation and authorization.\nFor example, restricting tool access to approved functions, validating parameters before execution.\nEnforcing rate limits and transaction caps for autonomous tool use.\nEstablishing execution monitoring and logging.\nFor example, tracking all tool calls, monitoring for unauthorized access attempts or scope violations.\nMaintaining decision boundary enforcement.\nFor example, limiting autonomous actions to defined parameters, requiring human approval for sensitive operations.\nReviewing patterns of AI tool usage for anomalies, updating tool permissions, and retiring unused or high-risk functions during scheduled evaluations.",
        "url_path": "https://aiuc-1.com/reliability/restrict-unsafe-tool-calls"
    },
    {
        "title": "D004. 3rd-party-testing of tool calls | AIUC-1",
        "section": "Reliability",
        "heading": "3rd-party-testing of tool calls",
        "description": "Appoint expert third-parties to evaluate tool calls in AI systems, including executing unauthorized actions, accessing restricted information, or making decisions beyond their intended scope at least every 3 months",
        "keywords": "Tool Calls; Tool Selection; Third-Party Testing",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10",
        "control_activities": "Should include\nAppointing qualified third-party assessors.\nFor example, selecting assessors with relevant technical capabilities for identified risk areas, maintaining records of assessor qualifications and independence.\nConducting regular testing.\nFor example, performing assessments of tool calls at least every quarter, defining testing scope and methodologies based on risk classifications.\nMaintaining documentation.\nFor example, recording third-party qualifications, testing scope, results, and remediation actions taken, tracking follow-up activities and resolution timelines.",
        "url_path": "https://aiuc-1.com/reliability/3rd-party-testing-of-tool-calls"
    },
    {
        "title": "D002. 3rd-party testing for hallucinations | AIUC-1",
        "section": "Reliability",
        "heading": "3rd-party testing for hallucinations",
        "description": "Appoint expert third-parties to evaluate hallucinated outputs at least every 3 months",
        "keywords": "Hallucinations; Third-Party Testing",
        "application": "Mandatory",
        "frequency": "Every 3 months",
        "type": "Preventative",
        "crosswalks": "NIST AI RMF; OWASP Top 10; EU AI Act",
        "control_activities": "Should include\nAppointing qualified third-party assessors.\nFor example, selecting assessors with relevant technical capabilities for identified risk areas, maintaining records of assessor qualifications and independence.\nConducting regular testing.\nFor example, performing assessments of hallucinated outputs at least every quarter, defining testing scope and methodologies based on risk classifications.\nMaintaining documentation.\nFor example, recording third-party qualifications, testing scope, results, and remediation actions taken, tracking follow-up activities and resolution timelines.",
        "url_path": "https://aiuc-1.com/reliability/3rd-party-testing-for-hallucinations"
    }
]