![](_page_0_Picture_0.jpeg)

# Crosswalk Between BSA Framework to Build Trust in AI and NIST AI Risk Management Framework

|  | BSA FRAMEWORK             |                                                                                                                                                                                                                                                                                                | NIST AI RISK MANAGEMENT FRAMEWORK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |  |
|--|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|
|  | GOVERNANCE                |                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |
|  | Policies and<br>Processes | Objectives: Integrate AI risk<br>management into broader risk<br>management functions.                                                                                                                                                                                                         | Sec. 1.2.4: Organizational Integration and Management of Risk: AI<br>risk management should be integrated and incorporated into broader<br>enterprise risk management strategies and processes. Treating AI risks<br>along with other critical risks, such as cybersecurity and privacy, will yield<br>a more integrated outcome and organizational efficiencies.                                                                                                                                                                                                                                                                                            |  |
|  |                           | Processes: Establish processes<br>for identifying risks, assessing<br>the materiality of those risks,<br>and mitigating risks at each<br>stage of the AI lifecycle.                                                                                                                            | Govern 1: Policies, processes, procedures, and practices across the<br>organization related to the mapping, measuring, and managing of AI<br>risks are in place, transparent, and implemented effectively.<br>Govern 4.3: Organizational practices are in place to enable AI testing,<br>identification of incidents, and information sharing.<br>Govern 6.2: Contingency processes are in place to handle failures or<br>incidents in third-party data or AI systems deemed to be high-risk.<br>Measure 3: Mechanisms for tracking identified AI risks over time are in<br>place.<br>Manage 3: AI risks and benefits from third-party entities are managed. |  |
|  |                           | Evaluation Mechanisms:<br>Establish mechanisms, such as<br>metrics and benchmarks, that<br>the organization will use to<br>evaluate whether policies and<br>procedures are being carried<br>out as specified.                                                                                  | Govern 1.5: Ongoing monitoring and periodic review of the risk<br>management process and its outcomes are planned and organizational<br>roles and responsibilities clearly defined, including determining the<br>frequency of periodic review.                                                                                                                                                                                                                                                                                                                                                                                                               |  |
|  |                           | Periodic Review: Organizations<br>should periodically review and<br>update their AI governance<br>framework so it remains<br>fit-for-purpose and capable<br>of addressing the evolving<br>landscape of risk.                                                                                   | Govern 1.5: Ongoing monitoring and periodic review of the risk<br>management process and its outcomes are planned and organizational<br>roles and responsibilities clearly defined, including determining the<br>frequency of periodic review.                                                                                                                                                                                                                                                                                                                                                                                                               |  |
|  |                           | Executive Oversight:<br>Governance framework should<br>be backed by executive<br>oversight, including<br>(1) approval of governance<br>policies, (2) active role<br>in overseeing product<br>development lifecycle, and (3)<br>accountability for go/no-go<br>decisions for high-risk systems. | Govern 2.3: Executive leadership of the organization takes responsibility<br>for decisions about risks associated with AI system development and<br>deployment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |  |

Governance Framework

### GOVERNANCE *(continued)*

# BSA FRAMEWORK NIST AI RISK MANAGEMENT FRAMEWORK

Personnel, Roles, and Responsibilities Independence: Personnel should be structured in a manner that facilitates separate layers of independent review. For example, risk management responsibilities may be split between a product development team, a compliance team, and a governance team. Govern 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks. Measure 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance. Competence, Resourcing, and Influence: Provide adequate training and resources for personnel to fulfill their governance functions and ensure that personnel are empowered to address and/or escalate risks. Govern 2.2: The organization's personnel and partners receive AI risk management training to enable them to perform their duties and responsibilities consistent with related policies, procedures, and agreements. Diversity: Establish team with diverse perspectives and lived experiences, and include traditionally underrepresented perspectives throughout the lifecycle of the AI design and development process. Where diversity is lacking on internal team, consult with external stakeholders as appropriate. Govern 3.1: Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, experience, expertise, and backgrounds). Map 1.2: Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary collaboration are prioritized. Measure 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance. Measure 4.1: Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Approaches are documented. Measure 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI actors to validate whether the system is performing consistently as intended. Results are documented. Measure 4.3: Measurable performance improvements or declines based on consultations with relevant AI actors, including affected communities, and field data about context-relevant risks and trustworthiness characteristics are identified and documented.

### PROJECT CONCEPTION

|  | Identify and<br>Document<br>Objectives and<br>Assumptions    | Document the intent and<br>purpose of the system.                                                      | Map 1.1: Intended purposes, potentially beneficial uses, context<br>specific laws, norms and expectations, and prospective settings in<br>which the AI system will be deployed are understood and documented.<br>Considerations include: the specific set or types of users along with<br>their expectations; potential positive and negative impacts of system<br>uses to individuals, communities, organizations, society, and the planet;<br>assumptions and related limitations about AI system purposes, uses, and<br>risks across the development or product AI lifecycle; and related TEVV<br>and system metrics.<br>Map 3: AI capabilities, targeted usage, goals, and expected benefits<br>and costs compared with appropriate benchmarks are understood. |
|--|--------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  |                                                              | Clearly define the model's<br>intended effects.                                                        | Map 1.1: Intended purposes, potentially beneficial uses, context<br>specific laws, norms and expectations, and prospective settings in<br>which the AI system will be deployed are understood and documented.<br>Considerations include: the specific set or types of users along with<br>their expectations; potential positive and negative impacts of system<br>uses to individuals, communities, organizations, society, and the planet;<br>assumptions and related limitations about AI system purposes, uses, and<br>risks across the development or product AI lifecycle; and related TEVV<br>and system metrics.                                                                                                                                           |
|  |                                                              |                                                                                                        | Map 2.1: The specific tasks and methods used to implement the tasks<br>that the AI system will support are defined (e.g., classifiers, generative<br>models, recommenders).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|  |                                                              |                                                                                                        | Map 3.3: Targeted application scope is specified and documented<br>based on the system's capability, established context, and AI system<br>categorization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|  |                                                              | Clearly define intended use<br>cases and context in which the<br>system will be deployed.              | Map 1.1: Intended purposes, potentially beneficial uses, context<br>specific laws, norms and expectations, and prospective settings in<br>which the AI system will be deployed are understood and documented.<br>Considerations include: the specific set or types of users along with<br>their expectations; potential positive and negative impacts of system<br>uses to individuals, communities, organizations, society, and the planet;<br>assumptions and related limitations about AI system purposes, uses, and<br>risks across the development or product AI lifecycle; and related TEVV<br>and system metrics.                                                                                                                                           |
|  |                                                              |                                                                                                        | Map 1.4: The business value or context of business use has been clearly<br>defined or—in the case of assessing existing AI systems—re-evaluated.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|  | Select and<br>Document Metrics<br>for Evaluating<br>Fairness | Identify "fairness" metrics that<br>will be used as a baseline for<br>assessing bias in the AI system. | Measure 1: Appropriate methods and metrics are identified and<br>applied.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|  | Document<br>Stakeholder<br>Impacts                           | Identify stakeholder groups<br>that may be impacted by the<br>system.                                  | Map 5: Impacts to individuals, groups, communities, organizations, and<br>society are characterized.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |

### PROJECT CONCEPTION *(continued)*

| Impact Assessment | Document<br>Stakeholder<br>Impacts<br>(continued) | For each stakeholder group,<br>document the potential<br>benefits and potential adverse<br>impacts, considering both the<br>intended uses and reasonably<br>foreseeable misuses of the<br>system. |  | Map 1.1: Intended purposes, potentially beneficial uses, context<br>specific laws, norms and expectations, and prospective settings in<br>which the AI system will be deployed are understood and documented.<br>Considerations include: the specific set or types of users along with<br>their expectations; potential positive and negative impacts of system<br>uses to individuals, communities, organizations, society, and the planet;<br>assumptions and related limitations about AI system purposes, uses, and<br>risks across the development or product AI lifecycle; and related TEVV<br>and system metrics.<br>Map 5.1: Likelihood and magnitude of each identified impact (both<br>potentially beneficial and harmful) based on expected use, past uses of<br>AI systems in similar contexts, public incident reports, feedback from<br>those external to the team that developed or deployed the AI system,<br>or other data are identified and documented.<br>Map 5.3: Practices and personnel for supporting regular engagement<br>with relevant AI actors and integrating feedback about positive,<br>negative, and unanticipated impacts are in place and documented. |
|-------------------|---------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                   |                                                   | Assess whether the nature of<br>the system makes it prone to<br>potential bias-related harms<br>based on user demographics.                                                                       |  | Measure 2: AI systems are evaluated for trustworthy characteristics.<br>Measure 2.3: AI system performance or assurance criteria are measured<br>qualitatively or quantitatively and demonstrated for conditions similar to<br>deployment setting(s). Measures are documented.<br>Measure 2.4: The functionality and behavior of the AI system and its<br>components—as identified in the MAP function—are monitored when in<br>production.<br>Measure 2.5: The AI system to be deployed is demonstrated to be valid<br>and reliable. Limitations of the generalizability beyond the conditions<br>under which the technology was developed are documented.<br>Measure 2.8: Risks associated with transparency and accountability—as<br>identified in the MAP function—are examined and documented.<br>Measure 2.9: The AI model is explained, validated, and documented,<br>and AI system output is interpreted within its context—as identified in<br>the MAP function—to inform responsible use and governance.<br>Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented.                                                    |
|                   | Document Risk<br>Mitigations                      | If risk of bias is present,<br>document efforts to mitigate<br>risks.                                                                                                                             |  | Manage 1: AI risks based on assessments and other analytical output<br>from the MAP and MEASURE functions are prioritized, responded to,<br>and managed.<br>Manage 1.3: Responses to the AI risks deemed high priority, as<br>identified by the MAP function, are developed, planned, and<br>documented. Risk response options can include mitigating, transferring,<br>avoiding, or accepting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |

## PROJECT CONCEPTION *(continued)*

| Impact Assessment              | Document Risk<br>Mitigations<br>(continued) | Document how identified risks<br>and potential harms of each<br>risk will be measured and how<br>the effectiveness of mitigation<br>strategies will be evaluated. | Measure 1: Appropriate methods and metrics are identified and<br>applied.<br>Measure 1.1: Approaches and metrics for measurement of AI risks<br>enumerated during the MAP function are selected for implementation<br>starting with the most significant AI risks. The risks or trustworthiness<br>characteristics that will not—or cannot—be measured are properly<br>documented.<br>Measure 1.2: Appropriateness of AI metrics and effectiveness of<br>existing controls are regularly assessed and updated, including reports<br>of errors and potential impacts on affected communities.<br>Measure 4: Feedback about efficacy of measurement is gathered and<br>assessed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|--------------------------------|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                |                                             | If risk of bias is present,<br>document efforts to mitigate<br>risk.                                                                                              | Manage 1: AI risks based on assessments and other analytical output<br>from the MAP and MEASURE functions are prioritized, responded to,<br>and managed.<br>Manage 1.3: Responses to the AI risks deemed high priority, as<br>identified by the MAP function, are developed, planned, and<br>documented. Risk response options can include mitigating, transferring,<br>avoiding, or accepting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|                                |                                             | If risks are unmitigated,<br>document why the risk was<br>deemed acceptable.                                                                                      | Manage 1.4: Negative residual risks (defined as the sum of all<br>unmitigated risks) to both downstream acquirers of AI systems and end<br>users are documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Risk Mitigation Best Practices | Independence<br>and Diversity               | Seek feedback from a diverse<br>set of stakeholders to inform<br>the impact assessment.                                                                           | Govern 3.1: Decision-making related to mapping, measuring, and<br>managing AI risks throughout the lifecycle is informed by a diverse team<br>(e.g., diversity of demographics, disciplines, experience, expertise, and<br>backgrounds).<br>Govern 5.1: Organizational policies and practices are in place to collect,<br>consider, prioritize, and integrate feedback from those external to the<br>team that developed or deployed the AI system regarding the potential<br>individual and societal impacts related to AI risks.<br>Map 1.2: Interdisciplinary AI actors, competencies, skills, and capacities<br>for establishing context reflect demographic diversity and broad domain<br>and user experience expertise, and their participation is documented.<br>Opportunities for interdisciplinary collaboration are prioritized.<br>Measure 1.3: Internal experts who did not serve as front-line developers<br>for the system and/or independent assessors are involved in regular<br>assessments and updates. Domain experts, users, AI actors external<br>to the team that developed or deployed the AI system, and affected<br>communities are consulted in support of assessments as necessary per<br>organizational risk tolerance.<br>Measure 4.1: Measurement approaches for identifying AI risks are<br>connected to deployment context(s) and informed through consultation<br>with domain experts and other end users. Approaches are documented.<br>Measure 4.2: Measurement results regarding AI system trustworthiness<br>in deployment context(s) and across the AI lifecycle are informed<br>by input from domain experts and relevant AI actors to validate<br>whether the system is performing consistently as intended. Results are<br>documented.<br>Measure 4.3: Measurable performance improvements or declines based<br>on consultations with relevant AI actors, including affected communities,<br>and field data about context-relevant risks and trustworthiness<br>characteristics are identified and documented. |

## PROJECT CONCEPTION *(continued)*

| Risk Mitigation Best Practices | Transparent<br>Documentation     | Share impact assessment<br>documentation with personnel<br>working on later stages of the<br>AI pipeline so that risks and<br>potential unintended impacts<br>can be monitored throughout<br>the development process. | Govern 4.2: Organizational teams document the risks and potential<br>impacts of the AI technology they design, develop, deploy, evaluate,<br>and use, and they communicate about the impacts more broadly.<br>Measure 1.3: Internal experts who did not serve as front-line developers<br>for the system and/or independent assessors are involved in regular<br>assessments and updates. Domain experts, users, AI actors external<br>to the team that developed or deployed the AI system, and affected<br>communities are consulted in support of assessments as necessary per<br>organizational risk tolerance. |
|--------------------------------|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                | Accountability<br>and Governance | Ensure that senior leadership<br>has been adequately briefed on<br>potential high-risk AI systems.                                                                                                                    | Govern 2.3: Executive leadership of the organization takes responsibility<br>for decisions about risks associated with AI system development and<br>deployment.                                                                                                                                                                                                                                                                                                                                                                                                                                                     |

### DATA ACQUISITION

| Impact Assessment              | Maintain<br>Records of Data<br>Provenance | Maintain sufficient records<br>to enable "recreation" of<br>the data used to train the AI<br>model, verify that its results<br>are reproducible, and monitor<br>for material updates to data<br>sources. | Govern 4.2: Organizational teams document the risks and potential<br>impacts of the AI technology they design, develop, deploy, evaluate,<br>and use, and they communicate about the impacts more broadly.<br>Manage 3.2: Pre-trained models which are used for development are<br>monitored as part of AI system regular monitoring and maintenance.                                                 |
|--------------------------------|-------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                | Examine Data for<br>Potential Biases      | Scrutinize data for historical<br>biases.                                                                                                                                                                | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.                                                                                                                  |
|                                |                                           | Evaluate "representativeness"<br>of the data.                                                                                                                                                            | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.                                                                                                                  |
|                                |                                           | Scrutinize data labeling<br>methodology.                                                                                                                                                                 | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.                                                                                                                  |
|                                | Document Risk<br>Mitigations              | Document whether and<br>how data was augmented,<br>manipulated, or re-balanced to<br>mitigate bias.                                                                                                      | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.                                                                                                                  |
|                                | Independence<br>and Diversity             | To facilitate robust interrogation<br>of the datasets, data review<br>teams should include personnel<br>that are diverse in terms of their<br>subject matter expertise and<br>lived experiences.         | Govern 3.1: Decision-making related to mapping, measuring, and<br>managing AI risks throughout the lifecycle is informed by a diverse team<br>(e.g., diversity of demographics, disciplines, experience, expertise, and<br>backgrounds).                                                                                                                                                              |
| Risk Mitigation Best Practices |                                           |                                                                                                                                                                                                          | Map 1.2: Interdisciplinary AI actors, competencies, skills, and capacities<br>for establishing context reflect demographic diversity and broad domain<br>and user experience expertise, and their participation is documented.<br>Opportunities for interdisciplinary collaboration are prioritized.                                                                                                  |
|                                |                                           |                                                                                                                                                                                                          | Measure 1.3: Internal experts who did not serve as front-line developers<br>for the system and/or independent assessors are involved in regular<br>assessments and updates. Domain experts, users, AI actors external<br>to the team that developed or deployed the AI system, and affected<br>communities are consulted in support of assessments as necessary per<br>organizational risk tolerance. |

### DATA ACQUISITION *(continued)*

| Risk Mitigation Best Practices | Re-Balancing<br>Unrepresentative<br>Data | Consider re-balancing with<br>additional data.                              | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.<br>Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented. |  |
|--------------------------------|------------------------------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|
|                                |                                          | Consider re-balancing with<br>synthetic data.                               | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.<br>Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented. |  |
|                                | Data Labeling                            | Establish objective and scalable<br>labeling guidelines.                    | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.                                                                                                                    |  |
|                                | Accountability<br>and Governance         | Integrate data labeling<br>processes into a comprehensive<br>data strategy. | Map 2.3: Scientific integrity and TEVV considerations are identified<br>and documented, including those related to experimental design, data<br>collection and selection (e.g., availability, representativeness, suitability),<br>system trustworthiness, and construct validation.                                                                                                                    |  |
|                                | DATA PREPARATION AND MODEL DEFINITION    |                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                         |  |

| Impact Assessment | Document<br>Feature Selection<br>and Engineering<br>Processes | Document rationale for choices<br>made during the feature<br>selection and engineering<br>processes and evaluate their<br>impact on model performance. | Govern 1: Policies, processes, procedures, and practices across the<br>organization related to the mapping, measuring, and managing of AI<br>risks are in place, transparent, and implemented effectively.                                                                                                                                                                                                                                                               |
|-------------------|---------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                   |                                                               | Document potential correlation<br>between selected features<br>and sensitive demographic<br>attributes.                                                | Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented.                                                                                                                                                                                                                                                                                                                                                          |
|                   | Document Model<br>Selection Process                           | Document rationale for the<br>selected modeling approach.                                                                                              | Govern 1.4: The risk management process and its outcomes are<br>established through transparent policies, procedures, and other controls<br>based on organizational risk priorities.<br>Map 2.1: The specific tasks and methods used to implement the tasks<br>that the AI system will support are defined (e.g., classifiers, generative<br>models, recommenders).                                                                                                      |
|                   |                                                               | Identify, document, and justify<br>assumptions in the selected<br>approach and potential<br>resulting limitations.                                     | Govern 1.4: The risk management process and its outcomes are<br>established through transparent policies, procedures, and other controls<br>based on organizational risk priorities.<br>Map 2.2: Information about the AI system's knowledge limits and how<br>system output may be utilized and overseen by humans is documented.<br>Documentation provides sufficient information to assist relevant AI<br>actors when making decisions and taking subsequent actions. |

|  | Feature Selection             | Examine for biased proxy<br>features.                                                                         | Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|--|-------------------------------|---------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  |                               | Scrutinize features that correlate<br>to sensitive attributes.                                                | Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|  | Independence<br>and Diversity | Seek feedback from diverse<br>stakeholders with domain<br>specific expertise.                                 | Govern 3.1: Decision-making related to mapping, measuring, and<br>managing AI risks throughout the lifecycle is informed by a diverse team<br>(e.g., diversity of demographics, disciplines, experience, expertise, and<br>backgrounds).<br>Map 1.2: Interdisciplinary AI actors, competencies, skills, and capacities<br>for establishing context reflect demographic diversity and broad domain<br>and user experience expertise, and their participation is documented.<br>Opportunities for interdisciplinary collaboration are prioritized.<br>Measure 1.3: Internal experts who did not serve as front-line developers<br>for the system and/or independent assessors are involved in regular<br>assessments and updates. Domain experts, users, AI actors external<br>to the team that developed or deployed the AI system, and affected<br>communities are consulted in support of assessments as necessary per<br>organizational risk tolerance.<br>Measure 4.1: Measurement approaches for identifying AI risks are<br>connected to deployment context(s) and informed through consultation<br>with domain experts and other end users. Approaches are documented.<br>Measure 4.2: Measurement results regarding AI system trustworthiness<br>in deployment context(s) and across the AI lifecycle are informed<br>by input from domain experts and relevant AI actors to validate<br>whether the system is performing consistently as intended. Results are<br>documented.<br>Measure 4.3: Measurable performance improvements or declines based |
|  |                               |                                                                                                               | on consultations with relevant AI actors, including affected communities,<br>and field data about context-relevant risks and trustworthiness<br>characteristics are identified and documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|  | Model Selection               | Avoid inscrutable models in<br>circumstances where both the<br>risk and potential impact of bias<br>are high. | No equivalent category                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |

### DATA PREPARATION AND MODEL DEFINITION *(continued)*

# VALIDATING, TESTING, AND REVISING THE MODEL

| Document<br>Validation<br>Processes | Document how the system (and<br>individual components) will be<br>validated to evaluate whether<br>it is performing consistent<br>with the design objectives and<br>intended deployment scenarios. | Measure 1: Appropriate methods and metrics are identified and<br>applied.<br>Measure 2.3: AI system performance or assurance criteria are measured<br>qualitatively or quantitatively and demonstrated for conditions similar to<br>deployment setting(s). Measures are documented.<br>Measure 2.4: The functionality and behavior of the AI system and its<br>components—as identified in the MAP function—are monitored when in<br>production.<br>Measure 2.5: The AI system to be deployed is demonstrated to be valid<br>and reliable. Limitations of the generalizability beyond the conditions<br>under which the technology was developed are documented.<br>Measure 2.9: The AI model is explained, validated, and documented,<br>and AI system output is interpreted within its context—as identified in<br>the MAP function—to inform responsible use and governance.<br>Measure 2.13: Effectiveness of the employed TEVV metrics and<br>processes in the MEASURE function are evaluated and documented. |
|-------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                     | Document re-validation<br>processes                                                                                                                                                                | Manage 2.2: Mechanisms are in place and applied to sustain the value<br>of deployed AI systems.<br>Manage 2.3: Procedures are followed to respond to and recover from a<br>previously unknown risk when it is identified.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Document Testing<br>Processes       | Test the system for bias by<br>evaluating and documenting<br>model performance.                                                                                                                    | Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|                                     | Document how testing was<br>performed, which fairness<br>metrics were evaluated, and<br>why those measures were<br>selected.                                                                       | Measure 2.1: Test sets, metrics, and details about the tools used during<br>TEVV are documented.<br>Measure 2.11: Fairness and bias—as identified in the MAP function—<br>are evaluated and results are documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|                                     | Document model interventions.                                                                                                                                                                      | Manage 1.3: Responses to the AI risks deemed high priority, as<br>identified by the MAP function, are developed, planned, and<br>documented. Risk response options can include mitigating, transferring,<br>avoiding, or accepting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|                                     |                                                                                                                                                                                                    | Manage 2.3: Procedures are followed to respond to and recover from a<br>previously unknown risk when it is identified.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|                                     |                                                                                                                                                                                                    | Manage 2.4: Mechanisms are in place and applied, and responsibilities<br>are assigned and understood, to supersede, disengage, or deactivate<br>AI systems that demonstrate performance or outcomes inconsistent with<br>intended use.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

### VALIDATING, TESTING, AND REVISING THE MODEL *(continued)*

| Risk Mitigation Best Practices | Model<br>Interventions        | Evaluate potential model<br>refinements to address bias<br>surfaced during testing.                                                 | Manage 1.3: Responses to the AI risks deemed high priority, as<br>identified by the MAP function, are developed, planned, and<br>documented. Risk response options can include mitigating, transferring,<br>avoiding, or accepting.<br>Manage 2.3: Procedures are followed to respond to and recover from a<br>previously unknown risk when it is identified.<br>Manage 2.4: Mechanisms are in place and applied, and responsibilities<br>are assigned and understood, to supersede, disengage, or deactivate<br>AI systems that demonstrate performance or outcomes inconsistent with<br>intended use. |
|--------------------------------|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                | Independence<br>and Diversity | Validation and testing<br>documentation should be<br>reviewed by personnel who<br>were not involved in the<br>system's development. | Measure 1.3: Internal experts who did not serve as front-line developers<br>for the system and/or independent assessors are involved in regular<br>assessments and updates. Domain experts, users, AI actors external<br>to the team that developed or deployed the AI system, and affected<br>communities are consulted in support of assessments as necessary per<br>organizational risk tolerance.                                                                                                                                                                                                   |

### PREPARING FOR DEPLOYMENT AND USE

| Impact Assessment | Document Lines<br>of Responsibility          | Define and document who is<br>responsible for the system's<br>outputs and the outcomes they<br>may lead to, including details<br>about how a system's decisions<br>can be reviewed if necessary.         | Govern 1.5: Ongoing monitoring and periodic review of the risk<br>management process and its outcomes are planned and organizational<br>roles and responsibilities clearly defined, including determining the<br>frequency of periodic review.<br>Govern 2.1: Roles and responsibilities and lines of communication<br>related to mapping, measuring, and managing AI risks are documented<br>and are clear to individuals and teams throughout the organization.                                                                                                                                                                                                                                                                                                                                                                |
|-------------------|----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                   |                                              | Establish management plans<br>for responding to potential<br>incidents or reports of system<br>errors.                                                                                                   | Manage 2.3: Procedures are followed to respond to and recover from a<br>previously unknown risk when it is identified.<br>Manage 2.4: Mechanisms are in place and applied, and responsibilities<br>are assigned and understood, to supersede, disengage, or deactivate<br>AI systems that demonstrate performance or outcomes inconsistent with<br>intended use.<br>Manage 4: Risk treatments, including response and recovery, and<br>communication plans for the identified and measured AI risks are<br>documented and monitored regularly.<br>Manage 4.1: Post-deployment AI system monitoring plans are<br>implemented, including mechanisms for capturing and evaluating<br>input from users and other relevant AI actors, appeal and override,<br>decommissioning, incident response, recovery, and change<br>management. |
|                   | Document<br>Processes for<br>Monitoring Data | Document what processes<br>and metrics will be used to<br>evaluate whether production<br>data (i.e., input data the system<br>encounters during deployment)<br>differs materially from training<br>data. | Manage 4.1: Post-deployment AI system monitoring plans are<br>implemented, including mechanisms for capturing and evaluating<br>input from users and other relevant AI actors, appeal and override,<br>decommissioning, incident response, recovery, and change<br>management.<br>Measure 4.2: Measurement results regarding AI system trustworthiness<br>in deployment context(s) and across the AI lifecycle are informed<br>by input from domain experts and relevant AI actors to validate<br>whether the system is performing consistently as intended. Results are<br>documented.                                                                                                                                                                                                                                          |

# PREPARING FOR DEPLOYMENT AND USE *(continued)*

| Impact Assessment              | Document<br>Processes for<br>Monitoring Model<br>Performance | For static models, document<br>how performance levels<br>and classes of error will be<br>monitored over time and<br>benchmarks that will trigger<br>review.                                                                                                                                                                        | Manage 3.2: Pre-trained models which are used for development are<br>monitored as part of AI system regular monitoring and maintenance.<br>Manage 4.1: Post-deployment AI system monitoring plans are<br>implemented, including mechanisms for capturing and evaluating<br>input from users and other relevant AI actors, appeal and override,<br>decommissioning, incident response, recovery, and change<br>management.                                                                                                                                                                                                                                                                    |
|--------------------------------|--------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                |                                                              | For models that are intended<br>to evolve over time,<br>document how changes will<br>be inventoried; if, when,<br>and how versions will be<br>captured and managed; and<br>how performance levels will<br>be monitored (e.g., cadence<br>of scheduled reviews,<br>performance indicators that<br>may trigger out-of-cycle review). | Manage 4.1: Post-deployment AI system monitoring plans are<br>implemented, including mechanisms for capturing and evaluating<br>input from users and other relevant AI actors, appeal and override,<br>decommissioning, incident response, recovery, and change<br>management.                                                                                                                                                                                                                                                                                                                                                                                                               |
|                                | Document Audit<br>and End-of-Life<br>Processes               | Document the cadence at which<br>impact assessment evaluations<br>will be audited to evaluate<br>whether risk mitigation controls<br>remain fit-for-purpose.                                                                                                                                                                       | Govern 1.5: Ongoing monitoring and periodic review of the risk<br>management process and its outcomes are planned and organizational<br>roles and responsibilities clearly defined, including determining the<br>frequency of periodic review.                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                                |                                                              | Document expected timeline<br>that system support will be<br>provided and processes for<br>decommissioning system<br>in event that it falls below<br>reasonable performance<br>thresholds.                                                                                                                                         | Govern 1.7: Processes and procedures are in place for<br>decommissioning and phasing out AI systems safely and in a<br>manner that does not increase risks or decrease the organization's<br>trustworthiness.<br>Manage 4.1: Post-deployment AI system monitoring plans are<br>implemented, including mechanisms for capturing and evaluating<br>input from users and other relevant AI actors, appeal and override,<br>decommissioning, incident response, recovery, and change<br>management.                                                                                                                                                                                              |
| Risk Mitigation Best Practices | Monitoring for<br>Drift and Model<br>Degradation             | Input data encountered<br>during deployment can be<br>evaluated against a statistical<br>representation of the system's<br>training data to evaluate<br>the potential for data drift<br>(i.e., material differences<br>between the training data<br>and deployment data that can<br>degrade model performance).                    | Measure 1.2: Appropriateness of AI metrics and effectiveness of<br>existing controls are regularly assessed and updated, including reports<br>of errors and potential impacts on affected communities.<br>Manage 2.2: Mechanisms are in place and applied to sustain the value<br>of deployed AI systems.<br>Manage 4.2: Measurable activities for continual improvements are<br>integrated into AI system updates and include regular engagement with<br>interested parties, including relevant AI actors.                                                                                                                                                                                  |
|                                | Product Features<br>and User Interface                       | Integrate product and user<br>interface features to mitigate<br>risk of foreseeable unintended<br>uses (e.g., interface that<br>enforces human-in-the-loop<br>requirements, alerts to notify<br>when a system is being<br>misused).                                                                                                | Manage 1.4: Negative residual risks (defined as the sum of all<br>unmitigated risks) to both downstream acquirers of AI systems and<br>end users are documented.<br>Manage 2.4: Mechanisms are in place and applied, and responsibilities<br>are assigned and understood, to supersede, disengage, or deactivate<br>AI systems that demonstrate performance or outcomes inconsistent with<br>intended use.<br>Manage 4.1: Post-deployment AI system monitoring plans are<br>implemented, including mechanisms for capturing and evaluating<br>input from users and other relevant AI actors, appeal and override,<br>decommissioning, incident response, recovery, and change<br>management. |

### PREPARING FOR DEPLOYMENT AND USE *(continued)*

|  | System<br>Documentation                         | AI Developers should provide<br>sufficient documentation<br>regarding system capabilities,<br>specifications, limitations,<br>and intended uses to enable<br>AI Deployers to perform<br>independent impact assessment<br>concerning deployment risks. | No equivalent category                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|--|-------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  |                                                 | Consider incorporating terms<br>into the End User License<br>Agreement that set forth<br>limitations designed to prevent<br>foreseeable misuses (e.g.,<br>contractual obligations to<br>ensure end user will comply<br>with acceptable use policy).   | No equivalent category                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|  |                                                 | Sales and marketing materials<br>should be closely reviewed to<br>ensure that they are consistent<br>with the system's actual<br>capabilities.                                                                                                        | No equivalent category                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|  | AI User Training                                | AI Deployers should provide<br>training for AI Users regarding<br>a system's capabilities and<br>limitations, and how outputs<br>should be evaluated and<br>integrated into a workflow.                                                               | Govern 2.2: The organization's personnel and partners receive AI<br>risk management training to enable them to perform their duties<br>and responsibilities consistent with related policies, procedures, and<br>agreements.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|  | Incident Response<br>and Feedback<br>Mechanisms | AI Deployers should maintain<br>a feedback mechanism to<br>enable AI Users and Affected<br>Individuals (i.e., members of the<br>public who may interact with<br>the system) to report concerns<br>about the operation of a<br>system.                 | Govern 5.2: Mechanisms are established to enable the team<br>that developed or deployed AI systems to regularly incorporate<br>adjudicated feedback from relevant AI actors into system design and<br>implementation.<br>Map 5.2: Practices and personnel for supporting regular engagement<br>with relevant AI actors and integrating feedback about positive,<br>negative, and unanticipated impacts are in place and documented.<br>Measure 3.3: Feedback processes for end users and impacted<br>communities to report problems and appeal system outcomes are<br>established and integrated into AI system evaluation metrics.<br>Measure 4: Feedback about efficacy of measurement is gathered and<br>assessed.<br>Manage 4: Risk treatments, including response and recovery, and<br>communication plans for the identified and measured AI risks are<br>documented and monitored regularly. |