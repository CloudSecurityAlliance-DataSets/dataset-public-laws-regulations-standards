#### **UC BERKELEY**

#### **CENTER FOR LONG-TERM CYBERSECURITY**

![](_page_0_Picture_2.jpeg)

CLTC WHITE PAPER SERIES

# A Taxonomy of Trustworthiness for Artificial Intelligence

**CONNECTING PROPERTIES OF TRUSTWORTHINESS WITH RISK MANAGEMENT AND THE AI LIFECYCLE**

JESSICA NEWMAN

#### CLTC WHITE PAPER SERIES

# A Taxonomy of Trustworthiness for Artificial Intelligence

#### **CONNECTING PROPERTIES OF TRUSTWORTHINESS WITH**

#### **RISK MANAGEMENT AND THE AI LIFECYCLE**

This resource includes the Taxonomy of Trustworthiness for Artificial Intelligence, which is discussed in more detail in the full paper available here: [https://cltc.berkeley.edu/wp-content/](https://cltc.berkeley.edu/wp-content/uploads/2023/01/Taxonomy_of_AI_Trustworthiness.pdf) [uploads/2023/01/Taxonomy_of_AI_Trustworthiness.pdf](https://cltc.berkeley.edu/wp-content/uploads/2023/01/Taxonomy_of_AI_Trustworthiness.pdf). The full paper also includes discussion of the term "trustworthy AI," compares multiple existing frameworks for trustworthy AI that informed this work, introduces the concept of properties of trustworthiness for AI, and includes in the appendix connections to international AI standards and a list of the properties of trustworthiness without segmentation by lifecycle stage.

JESSICA NEWMAN

JANUARY 2023

![](_page_1_Picture_7.jpeg)

# Introduction

The National Institute of Standards and Technology (NIST) has developed an AI Risk Management Framework (RMF) intended to promote trustworthy artificial intelligence (AI). In this report, we introduce a taxonomy of trustworthiness for artificial intelligence that is intended to complement and support the use of the NIST AI RMF.

The taxonomy includes 150 properties of trustworthiness for AI. Each property builds upon a relevant "characteristic of trustworthiness" as defined by NIST in the AI RMF. NIST's characteristics of trustworthiness include: valid and reliable; safe, secure, and resilient; accountable and transparent; explainable and interpretable; privacy-enhanced; and fair with harmful bias managed.

The taxonomy is organized by the seven stages of the AI lifecycle depicted in the NIST AI RMF:

- Plan and Design
A Taxonomy of Trustworthiness

for Artificial Intelligence

**CONNECTING PROPERTIES OF TRUSTWORTHINESS WITH** 

**RISK MANAGEMENT AND THE AI LIFECYCLE**

- Collect and Process Data
- Build and Use Model
- Verify and Validate
- Deploy and Use
- Operate and Monitor
- Use or Impacted By

Our hope is that this framework supports usability by connecting the taxonomy more closely to actual product cycles and workflows. We also hope to provide ideas about possible ways to connect the NIST AI RMF Core to the AI lifecycle.

Within each stage of the lifecycle, the taxonomy includes NIST's seven characteristics of trustworthiness. These categories are further broken down to include all the properties of trustworthiness that are relevant at that stage of the lifecycle.

We also include an eighth characteristic of trustworthiness in the taxonomy, which varies slightly from the NIST AI RMF. The additional characteristic is "Responsible Practice and Use." The NIST AI RMF recognizes its importance and states that "AI risk management can drive responsible uses and practices," but does not include it as a characteristic of trustworthiness. In this paper, we include it as a crosscutting characteristic of trustworthiness because we find

that it serves a critical role in highlighting the interconnected nature of AI technologies with the people, organizations, and structures that are designing, building, and deploying them. We use Responsible Practice and Use in this taxonomy to promote consistent understanding of AI as a sociotechnical system, situated within structures of practice and use.

Each property is accompanied by a set of questions to guide initial thinking. For example, the "Data Protection" property includes the question, "How will we use encryption, differential privacy, federated learning, data minimization, and/or other best practices to protect data?" The questions are formulated in this future-oriented way (and not as "Have we . . . ?") because they are intended to serve as a tool to spark further discussion and action, rather than as a checklist or a scorecard.

Each property included in the taxonomy is also tagged with a set of subcategories from the NIST AI RMF. These subcategories represent the most relevant sections of the NIST AI RMF Core framework. Reviewing these subcategories will point a reader to helpful resources and tools to address the property. A small number of the listed subcategories (in most cases just one or two) are bolded to emphasize that they are likely to be particularly helpful or a good place to start. There may be additional subcategories not listed here that are also relevant, depending on the context of an AI system's development and use.

Finally, the taxonomy was developed to be useful for understanding a full spectrum of AI systems, including those that have limited engagement with people, which have typically been underemphasized in considerations of AI trustworthiness. A subset of the properties of trustworthiness in the taxonomy are likely to only be relevant to AI systems that are humanfacing, which may engage directly with human users or operators, make use of human data, or inform human decision-making. These properties are marked in the table with an asterisk after their name. Properties that do not have an asterisk are likely to be relevant to AI systems across the spectrum of human-AI engagement.

This taxonomy aims to provide a resource that is useful for AI organizations and teams developing AI technologies, systems, and applications. While it is designed specifically to assist users of the NIST AI RMF, it could also be helpful for people using any kind of AI risk or impact assessment, or for people developing model cards, system cards, or other types of AI documentation. It may also be useful for standards-setting bodies, policymakers, independent auditors, and civil society organizations working to evaluate and promote trustworthy AI.

**[FULL REPORT AVAILABLE HERE](https://cltc.berkeley.edu/wp-content/uploads/2023/01/Taxonomy_of_AI_Trustworthiness.pdf)**

# Taxonomy of Trustworthiness for Artificial Intelligence

# *AI Lifecycle Stage: Plan and Design*

The purpose of the plan and design stage is to articulate and document the system's concept and objectives, underlying assumptions, context, and requirements.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness   | Question(s) to Consider                                                                                                                                                                                                       | Relevant NIST AI RMF<br>Subcategories                                                                                                                               |
|--------------------------------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Valid and Reliable                         | Fit for Purpose                    | How will we assess whether the AI system<br>is fit for purpose for each intended use and<br>provides a valid solution for the problems we<br>are trying to solve? How will we ensure that<br>inappropriate uses are rejected? | Govern 5.1<br>Map 1.1<br>Map 1.2<br>Map 1.3<br>Map 1.4<br>Map 3.1<br>Map 3.2<br>Map 3.3<br>Manage 1.1                                                               |
|                                            | Predictable and<br>Dependable      | How will we ensure that the AI system will<br>behave as expected? If the AI system is<br>not fully predictable, how will we assess<br>whether it can still be depended upon for<br>our purposes?                              | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Map 2.2<br>Map 2.3<br>Measure 2.3<br>Measure 2.4<br>Measure 2.5<br>Measure 2.6<br>Measure 2.7<br>Manage 2.4<br>Manage 4.1 |
|                                            | Appropriate Level of<br>Automation | How will we determine the desired and<br>appropriate degree of automation, given the<br>AI system's characteristics and the context<br>of its uses?                                                                           | Govern 3.2<br>Map 1.1<br>Map 1.2<br>Map 1.3<br>Map 2.2<br>Map 3.5<br>Measure 4.2<br>Manage 1.1<br>Manage 4.1                                                        |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                      | Question(s) to Consider                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                            |
|--------------------------------------------|-------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | High Quality AI<br>System Configuration               | How will we assess the quality of the AI<br>system design and configuration and ensure<br>consistently high quality? For example, how<br>will we assess and ensure the quality of all<br>of the software components integrated<br>into the AI system? How will we assess and<br>ensure the quality of the hardware for the AI<br>system, such as AI chips, including graphics<br>processing units (GPUs), field-programmable<br>gate arrays (FPGAs), and application-specific<br>integrated circuits (ASICs)? | Govern 4.3<br>Govern 5.2<br>Govern 6.1<br>Map 1.6<br>Map 2.3<br>Measure 1.3<br>Measure 2.3<br>Measure 3.3<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1                                                              |
|                                            | High Quality Network<br>Resources and<br>Services     | How will we assess and ensure the quality of<br>shared network resources and services, e.g.,<br>distributed dataset access?                                                                                                                                                                                                                                                                                                                                                                                   | Govern 6.1<br>Govern 6.2<br>Map 2.3<br>Map 4.1<br>Map 4.2<br>Measure 2.3<br>Measure 2.4<br>Measure 3.1<br>Measure 3.3<br>Manage 3.1<br>Manage 3.2                                                                |
|                                            | Trusted Dependencies<br>on External Parties           | How will we identify, assess, and monitor our<br>dependencies on external parties?                                                                                                                                                                                                                                                                                                                                                                                                                            | Govern 6.1<br>Govern 6.2<br>Map 4.1<br>Map 4.2<br>Manage 3.1<br>Manage 3.2                                                                                                                                       |
|                                            | Foresight and<br>Scenario Planning                    | How will we assess and navigate possible<br>futures and the evolving risk landscape?                                                                                                                                                                                                                                                                                                                                                                                                                          | Govern 3.1<br>Govern 4.1<br>Map 1.1<br>Map 1.2<br>Map 3.1<br>Map 3.2<br>Measure 3.1<br>Measure 3.2                                                                                                               |
| Safe                                       | Protection of Physical<br>and Psychological<br>Safety | How will we ensure that the AI system will<br>not cause physical or psychological harm or<br>lead to a state in which human life, health,<br>property, or the environment is endangered?<br>How will we anticipate potential failure<br>modes or unsafe conditions?                                                                                                                                                                                                                                           | Govern 1.7<br>Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Govern 6.2<br>Map 1.1<br>Measure 1.2<br>Measure 1.3<br>Measure 2.6<br>Measure 3.1<br>Measure 3.3<br>Manage 2.4<br>Manage 4.1 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                          | Question(s) to Consider                                                                                                                                                                                                                                                        | Relevant NIST AI RMF<br>Subcategories                                                                                                                          |
|--------------------------------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Assurance /<br>Management of<br>Uncertainty                               | If we do not know all of the elements<br>required for the safe development and<br>deployment of the AI system, how will we<br>manage this uncertainty?                                                                                                                         | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Measure 2.6<br>Measure 3.2<br>Manage 2.3<br>Manage 4.1                                                               |
|                                            | Assurance /<br>Management of Multi<br>Capability / Multi<br>Modal Systems | If an AI system has multiple capabilities or<br>works across multiple modalities, how will<br>we document and manage this complexity?                                                                                                                                          | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Map 1.1<br>Map 2.2<br>Map 3.3<br>Measure 3.1<br>Measure 3.2<br>Measure 3.3<br>Manage 2.3<br>Manage 2.4<br>Manage 4.1 |
|                                            | Alignment with<br>Human Values                                            | How will we ensure that the AI system abides<br>by desired human values and does not<br>sacrifice human values to achieve its narrow<br>goals?                                                                                                                                 | Govern 3.1<br>Govern 4.1<br>Govern 4.2<br>Map 1.1<br>Map 1.2<br>Map 1.6<br>Map 3.5<br>Measure 2.6<br>Measure 3.1<br>Measure 3.3<br>Manage 4.1                  |
|                                            | Governable                                                                | How will we ensure an AI system is designed<br>and engineered to achieve its goals while<br>maintaining the ability to disengage or<br>deactivate the system if necessary? How<br>will we ensure an AI system would not have<br>incentives to resist or deceive its operators? | Govern 4.1<br>Map 2.2<br>Measure 2.4<br>Measure 2.5<br>Measure 2.6<br>Manage 2.4                                                                               |
| Fair with Harmful Bias<br>Managed          | Diverse                                                                   | How will we ensure that gender, racial,<br>age, ability, religious, cultural, disciplinary,<br>and other relevant types of diversity are<br>represented within the teams influencing AI<br>development and use, throughout all stages<br>of the AI lifecycle?                  | Govern 2.1<br>Govern 3.1<br>Govern 5.1<br>Map 1.2<br>Measure 1.3<br>Measure 2.2<br>Measure 4.2                                                                 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                                                                                                                                                           | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                                                 |
|--------------------------------------------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Inclusive                        | How will we ensure inclusivity of all relevant<br>experts and communities in the design and<br>development of the AI system?                                                                                                                                                                                      | Govern 2.1<br>Govern 3.1<br>Govern 3.2<br>Govern 5.1<br>Govern 5.2<br>Map 1.2<br>Measure 1.3<br>Measure 2.2<br>Measure 3.3<br>Measure 4.2<br>Manage 4.2                                                                               |
|                                            | Equitable                        | How will we navigate structural power<br>dynamics and promote equity in the design<br>and use of the AI system? (For example, how<br>will different communities be given power<br>to influence decisions? Who will experience<br>potential benefits of the AI system and who<br>will experience potential harms?) | Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.2<br>Measure 1.3<br>Measure 2.2<br>Measure 2.11<br>Measure 3.3<br>Measure 4.3<br>Manage 4.1                                           |
|                                            | Just                             | How will we ensure justice in the design and<br>use of the AI system? (For example, are all<br>the people involved in the training, design,<br>and development of the AI system treated<br>fairly, even in less visible roles, such as data<br>annotators?)                                                       | Govern 3.1<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.2<br>Measure 1.3<br>Measure 2.2<br>Measure 2.11<br>Measure 3.3<br>Measure 4.3<br>Manage 4.1<br>Manage 4.3 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness         | Question(s) to Consider                                                                                                                                                                                         | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                     |
|--------------------------------------------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Mitigation of Systemic<br>and Human Bias | How will we assess and mitigate ways<br>in which systemic and human bias may<br>influence the design, development, and<br>deployment of the AI system?                                                          | Govern 2.2<br>Govern 3.1<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 2.11<br>Measure 3.3<br>Measure 4.3<br>Manage 4.1 |
|                                            | Solidarity                               | How will we ensure the design and use of the<br>AI system respects the solidarity of groups<br>and communities, such as workers, women,<br>people with disabilities, ethnic minorities,<br>children, or others? | Govern 3.1<br>Govern 3.2<br>Govern 4.2<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Measure 4.3<br>Manage 4.1                 |
| Secure and Resilient                       | Security-by-Design                       | How will we build security into the AI system<br>design, testing, deployment, and operation?<br>How often will we provide security updates<br>to the AI system?                                                 | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Govern 6.1<br>Map 1.1<br>Map 1.6<br>Map 2.3<br>Map 4.2<br>Measure 2.7<br>Manage 2.4                                                             |
|                                            | Availability                             | How will we ensure that information for and<br>about the AI system is available to authorized<br>personnel when it is needed?                                                                                   | Govern 4.1<br>Govern 4.3<br>Govern 6.1<br>Govern 6.2<br>Map 1.1<br>Map 2.3<br>Measure 2.7<br>Measure 2.9                                                                                  |
|                                            | Confidentiality                          | How will we ensure that information is not<br>made available or disclosed to unauthorized<br>individuals, entities, or processes?                                                                               | Govern 4.1<br>Govern 4.3<br>Govern 6.1<br>Govern 6.2<br>Map 1.1<br>Map 2.3<br>Measure 2.7<br>Measure 2.10                                                                                 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                    | Question(s) to Consider                                                                                                                                                                                                            | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                  |
|--------------------------------------------|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Integrity                                           | How will we maintain and ensure the<br>accuracy, completeness, and appropriateness<br>of data, models, and procedures informing<br>the AI system?                                                                                  | Govern 4.1<br>Govern 4.3<br>Govern 6.1<br>Govern 6.2<br>Map 1.1<br>Map 2.3<br>Measure 2.7<br>Measure 2.9                                                                                               |
| Explainable and<br>Interpretable           | Intelligible*                                       | How will we assess the system for intelligible<br>explanations and select a model to support<br>this?                                                                                                                              | Map 1.1<br>Map 2.2<br>Measure 2.9                                                                                                                                                                      |
|                                            | Positive Human<br>Machine Interaction*              | How will we enable positive human-machine<br>interactions throughout the AI system's<br>operation?                                                                                                                                 | Govern 3.2<br>Map 1.1<br>Map 1.2<br>Map 2.2<br>Map 3.5<br>Map 5.2<br>Measure 2.9                                                                                                                       |
| Privacy-Enhanced                           | Privacy-by-Design*                                  | How will privacy be built into the AI system<br>design, testing, deployment, and operation?<br>If data includes sensitive or personally<br>identifiable information including biometrics,<br>what extra precautions will be taken? | Govern 1.1<br>Govern 1.2<br>Govern 6.1<br>Map 1.1<br>Map 4.1<br>Measure 2.10                                                                                                                           |
|                                            | Data Privacy or<br>Protection Impact<br>Assessment* | What is the impact of the AI system on<br>privacy? When and how will we conduct<br>a data privacy or data protection impact<br>assessment?                                                                                         | Govern 1.1<br>Govern 1.2<br>Govern 6.1<br>Map 1.1<br>Map 4.1<br>Measure 2.10<br>Manage 4.1                                                                                                             |
| Accountable and<br>Transparent             | Effective Policy and<br>Governance                  | How will we analyze and follow or implement<br>relevant or desired AI and data standards,<br>policies, principles, and guidance?                                                                                                   | Govern 1.1<br>Govern 1.2<br>Govern 1.3<br>Map 3.5<br>Map 4.1<br>Map 5.1<br>Map 5.2<br>Measure 1.1<br>Measure 1.2<br>Measure 1.3<br>Measure 2.8<br>Manage 1.3<br>Manage 2.1<br>Manage 3.1<br>Manage 4.1 |
|                                            | Adherence to the Rule<br>of Law                     | How will we analyze and ensure compliance<br>with all relevant laws and regulations across<br>every jurisdiction of use? How will we analyze<br>liability considerations, and what precautions<br>will be taken?                   | Govern 1.1<br>Map 4.1<br>Manage 1.3                                                                                                                                                                    |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                        | Question(s) to Consider                                                                                                                                                                                                                                                                                                                       | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                                            |
|--------------------------------------------|---------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Coordination (Public<br>Private; International)         | How will we identify and coordinate<br>with relevant institutions, nationally and<br>internationally?                                                                                                                                                                                                                                         | Govern 5.1<br>Govern 5.2<br>Map 5.2<br>Measure 1.3<br>Measure 4.1<br>Measure 4.2<br>Measure 4.3                                                                                                                                  |
|                                            | Effective Risk<br>Assessments and<br>Impact Assessments | How will we assess, document, and<br>communicate (on a regular basis) the<br>expected, potential, and actual risks and<br>impacts of the AI system on people,<br>organizations, and society (pre- and post<br>deployment)? If risks and impact are deemed<br>to be unacceptable, how will we ensure the<br>AI system is adjusted or rejected? | Govern 1.3<br>Govern 1.4<br>Govern 1.7<br>Govern 6.1<br>Map 1.1<br>Map 3.2<br>Map 5.1<br>Map 5.2<br>Measure 1.1<br>Measure 1.3<br>Manage 1.1<br>Manage 1.2<br>Manage 1.3<br>Manage 1.4<br>Manage 2.1<br>Manage 2.3<br>Manage 2.4 |
|                                            | Community<br>Engagement                                 | How will we identify communities interested<br>in, engaged in, or impacted by the AI<br>system, and how will we encourage their<br>participation throughout the AI lifecycle?                                                                                                                                                                 | Govern 5.1<br>Govern 5.2<br>Map 1.2<br>Map 5.2<br>Measure 3.3<br>Measure 4.1<br>Measure 4.2<br>Measure 4.3<br>Manage 4.2                                                                                                         |
|                                            | Open                                                    | How can we promote openness and<br>transparency about our development and<br>governance of AI technologies, internally and<br>externally?                                                                                                                                                                                                     | Govern 1.2<br>Govern 1.4<br>Govern 1.6<br>Govern 4.2<br>Govern 4.3<br>Map 5.2<br>Measure 1.3<br>Measure 2.9<br>Measure 2.8<br>Manage 4.3                                                                                         |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                                                                                                     | Question(s) to Consider                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Relevant NIST AI RMF<br>Subcategories                                                                                                                                           |
|--------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Documentation                                                                                                                                        | How will we document the AI system's<br>design, datasets, training, characteristics,<br>capabilities, limitations, predictable failures,<br>intended uses, etc.? How will we review and<br>update the documentation on a regular<br>basis and as needed to document new uses,<br>functionalities, etc.?                                                                                                                                                                  | Govern 1.6<br>Govern 4.2<br>Map 1.1<br>Map 2.3<br>Map 3.1<br>Map 3.2<br>Map 3.3<br>Map 3.4<br>Map 3.5<br>Map 4.1<br>Map 4.2<br>Map 5.1<br>Map 5.2<br>Measure 2.9<br>Measure 2.8 |
|                                            | Internal Reporting /<br>Culture of Safety                                                                                                            | How will we incentivize internal reporting<br>of challenges or concerns, and promote a<br>culture of safety among teams involved with<br>the AI system and in general?                                                                                                                                                                                                                                                                                                   | Govern 1.2<br>Govern 2.2<br>Govern 2.3<br>Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Measure 2.8                                                                                 |
|                                            | Internal Reviews                                                                                                                                     | How will internal reviews be conducted to<br>assess trustworthy AI practices?                                                                                                                                                                                                                                                                                                                                                                                            | Govern 1.5<br>Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Measure 2.8<br>Measure 2.13                                                                                             |
| Responsible Practice<br>and Use            | Responsible Use<br>in Government,<br>Education, Health,<br>Finance, Workplace,<br>Identification and<br>Detection, and other<br>High-stakes Settings | How will we ensure responsible potential and<br>actual uses in high-stakes settings, such as<br>government, education, healthcare, finance,<br>employment, workplace, identification and<br>detection (such as emotion detection), and<br>others? If our AI system influences one of<br>these domains, how will we ensure that we<br>engage sufficiently with domain experts and<br>impacted communities to better understand<br>the influence and impact we might have? | A majority of all of the<br>subcategories are critical.<br>Map 1.1 is especially<br>relevant to help<br>understand the purpose,<br>context, and impacts of<br>the intended use. |
|                                            | Responsible Use in<br>Critical Infrastructure<br>and Safety-Critical<br>Systems                                                                      | How will we ensure responsible potential<br>and actual uses for critical infrastructure and<br>safety-critical systems, including assessing<br>the potential for damaging effects from<br>technical faults, defects, or attacks?                                                                                                                                                                                                                                         | A majority of all of the<br>subcategories are critical.<br>Map 1.1 is especially<br>relevant to help<br>understand the purpose,<br>context, and impacts of<br>the intended use. |
|                                            | Responsible Use in<br>the Criminal Legal<br>System and by Law<br>Enforcement                                                                         | How will we ensure responsible potential<br>and actual uses in the criminal legal system<br>or by law enforcement? For example, how<br>will we protect against abuses of biometric<br>identification in public spaces?                                                                                                                                                                                                                                                   | A majority of all of the<br>subcategories are critical.<br>Map 1.1 is especially<br>relevant to help<br>understand the purpose,<br>context, and impacts of<br>the intended use. |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                                                                                  | Question(s) to Consider                                                                                                                                                             | Relevant NIST AI RMF<br>Subcategories                                                                                                                                              |
|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Responsible Use in<br>Defense and National<br>Security                                                                            | How will we promote peace and ensure<br>responsible and controlled uses for defense,<br>military, border control, and national security<br>purposes, including for weapons systems? | A majority of all of the<br>subcategories are critical.<br>Map 1.1 is especially<br>relevant to help<br>understand the purpose,<br>context, and impacts of<br>the intended use.    |
|                                            | Verified Supply Chain                                                                                                             | How will we assess and verify the relevant<br>components of the supply chain?                                                                                                       | Govern 6.1<br>Govern 6.2<br>Map 4.1<br>Map 4.2<br>Manage 3.1<br>Manage 3.2                                                                                                         |
|                                            | Appropriate<br>Assignment of<br>Organizational<br>Roles, Authorities,<br>and Responsibilities;<br>Designated Points of<br>Contact | How will we assign and document<br>organizational roles, authorities, and<br>responsibilities? How will we designate<br>points of contact along the lifecycle?                      | Govern 2.1<br>Govern 2.2<br>Govern 2.3<br>Govern 3.1<br>Govern 3.2<br>Map 3.4<br>Map 3.5<br>Manage 2.1                                                                             |
|                                            | Effective Capabilities                                                                                                            | How will we obtain the necessary resources<br>and knowledge to achieve our trustworthy<br>AI objectives?                                                                            | Govern 2.2<br>Map 3.4                                                                                                                                                              |
|                                            | Collaboration                                                                                                                     | How will we enable multi-stakeholder<br>collaboration?                                                                                                                              | Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.2<br>Map 5.2<br>Manage 4.2                                                                                                         |
|                                            | Supportive<br>Governance and<br>Organizational<br>Structure                                                                       | How can our governance and organizational<br>structure support trustworthy AI? How do<br>our strategy, objectives, and policies support<br>trustworthy AI? Are changes needed?      | Govern 1.1<br>Govern 1.2<br>Govern 1.3<br>Govern 1.4<br>Govern 1.5<br>Govern 1.6<br>Govern 1.7<br>Govern 2.1<br>Govern 2.2<br>Govern 2.3<br>Govern 4.1<br>Govern 4.2<br>Govern 4.3 |
|                                            | Effective Hiring and<br>Training                                                                                                  | How will we support the hiring and training<br>of individuals who can carry out trustworthy<br>AI objectives?                                                                       | Govern 2.1<br>Govern 2.2<br>Govern 2.3                                                                                                                                             |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                            | Question(s) to Consider                                                                                                                                                                             | Relevant NIST AI RMF<br>Subcategories                                                 |
|--------------------------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
|                                            | Responsible Labor<br>Practices and Rights                                   | How can we support labor rights in our use<br>of AI? How will the supply chain of the AI<br>system be monitored to evaluate working<br>conditions?                                                  | Govern 1.1<br>Govern 1.2<br>Govern 2.1<br>Govern 6.1<br>Map 1.1<br>Map 3.4<br>Map 5.2 |
|                                            | Leadership<br>Commitment                                                    | How will we ensure long-term commitment<br>to trustworthy AI from organizational<br>leadership?                                                                                                     | Govern 2.1<br>Govern 2.3                                                              |
|                                            | Supportive<br>Organizational Culture                                        | How will our organizational culture support<br>our trustworthy AI objectives? Are changes<br>needed?                                                                                                | Govern 1.2<br>Govern 1.4<br>Govern 2.2<br>Govern 2.3<br>Govern 4.1                    |
|                                            | Procurement<br>Standards                                                    | How will we implement/ensure AI<br>procurement standards that support<br>trustworthy AI if we are procuring the AI<br>system or providing it to others?                                             | Govern 1.2<br>Govern 4.2<br>Govern 6.1<br>Map 1.3<br>Map 1.4<br>Map 4.1<br>Map 4.2    |
|                                            | Appropriate<br>Relationships,<br>Interdependencies,<br>and Interconnections | What relationships, interdependencies, and<br>interconnections will be involved in the<br>development and use of the AI system, and<br>how do they intersect with our trustworthy<br>AI objectives? | Map 1.1<br>Map 4.1<br>Manage 3.1                                                      |
|                                            | Alignment with<br>Organizational Vision,<br>Mission, and Values             | How will we ensure the AI system is true to<br>our vision, mission, and values?                                                                                                                     | Govern 4.3<br>Govern 5.1<br>Map 1.1<br>Map 5.2<br>Measure 4.2<br>Manage 1.1           |
|                                            | Socially Responsible                                                        | How will our AI system and its use align with<br>our social responsibility efforts?                                                                                                                 | Govern 1.2<br>Govern 4.1<br>Map 1.1<br>Map 5.1<br>Map 5.2<br>Manage 1.1               |
|                                            | Supportive of Fair<br>Competition                                           | How will we support fair competition among<br>a variety of actors in the domain in which<br>our AI system is applied?                                                                               | Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 5.2<br>Manage 4.1                          |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                    | Question(s) to Consider                                                                                                                                                                                                                                                                                                                                      | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                                                             |
|--------------------------------------------|-----------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Supportive of Civil<br>Rights                       | How will we protect and promote civil<br>rights throughout the AI lifecycle, including<br>protection from unlawful discrimination<br>on the basis of race, color, national origin,<br>disability, age, religion, and sex (including<br>pregnancy, sexual orientation, and gender<br>identity)?                                                               | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 2.2<br>Measure 2.11<br>Measure 3.3<br>Manage 1.3<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3 |
|                                            | Supportive of<br>Democratic Values<br>and Processes | How will we ensure the design and use of the<br>AI system are consistent with democratic<br>values such as freedom and equality? How<br>will we ensure that the uses of the AI system<br>do not interfere with democratic processes<br>and citizens' rights, including the right to<br>vote? How will we assess the impact of the AI<br>system on democracy? | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Manage 1.3<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3                                               |
|                                            | Protection of Human<br>Autonomy and<br>Freedom      | How will we ensure that the AI system<br>respects the freedom and autonomy of<br>individuals and does not intrude on people's<br>self-determination and ability to make life<br>decisions for themselves?                                                                                                                                                    | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 3.5<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Manage 1.3<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3                                    |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                          |
|--------------------------------------------|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Protection of Human<br>Dignity   | How will we ensure that the development<br>and use of the AI system respect human<br>dignity and treat people as having intrinsic<br>worth, and not merely as objects?                                                                                                                                                                                                                                                                                                                                             | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 3.5<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Manage 1.3<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3 |
|                                            | Protection of Human<br>Rights    | How will we ensure the AI system does not<br>threaten human rights? For example, how<br>will we ensure the right to privacy? How will<br>we ensure the AI system does not pose risks<br>of gender or sexual violence? How will we<br>ensure it does not threaten children's rights?<br>How will we ensure the AI system does not<br>threaten freedom of religion, or freedom<br>of expression? How will we ensure the AI<br>system does not threaten the right to fair<br>trial or the right of peaceful assembly? | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 3.5<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Manage 1.3<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3 |
|                                            | Supportive of<br>Wellbeing       | How will we ensure the AI system supports<br>individual, community, and societal wellbeing,<br>including mental or emotional wellbeing?                                                                                                                                                                                                                                                                                                                                                                            | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 3.5<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Manage 1.3<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                                      | Question(s) to Consider                                                                                                                                                                                                                                                                            | Relevant NIST AI RMF<br>Subcategories                                                                     |
|--------------------------------------------|---------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
|                                            | Reduction of Carbon<br>Emissions                                                      | How can we reduce the carbon emissions<br>from the design and use of AI systems in<br>general?                                                                                                                                                                                                     | Govern 1.7<br>Govern 4.2<br>Map 1.1<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 2.12<br>Manage 4.1    |
|                                            | Assessment of<br>Economic, Social,<br>Cultural, Political, and<br>Global Implications | How will we assess the economic<br>implications of the AI system, including<br>whether use of the system could impact<br>jobs or reduce the need for human labor?<br>How will we assess the social, cultural, and<br>political implications of the AI system at the<br>societal and global levels? | Govern 3.1<br>Govern 5.1<br>Map 1.1<br>Map 1.2<br>Map 3.1<br>Map 3.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3 |

#### **AI LIFECYCLE STAGE: COLLECT AND PROCESS DATA**

The purpose of this stage is to collect and process data, including to gather, validate, and clean data and document the metadata and characteristics of the dataset.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                  | Question(s) to Consider                                                                                                                                                                                              | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                               |
|--------------------------------------------|-------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Valid and Reliable                         | Data Completeness                                                 | How will we assess and improve the<br>completeness, quantity, suitability, and<br>representativeness of the data?                                                                                                    | Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Map 1.1<br>Map 1.2<br>Map 2.3<br>Measure 2.2<br>Measure 2.11<br>Manage 3.1<br>Manage 3.2                                                                                  |
|                                            | Data Quality                                                      | How will we assess and improve the<br>quality and relevance of the data? What<br>benchmarks will we use? How will we collect<br>and process data, for example to annotate,<br>label, clean, and aggregate as needed? | Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Map 1.1<br>Map 1.2<br>Map 2.3<br>Measure 2.2<br>Manage 1.1<br>Manage 3.1<br>Manage 3.2                                                                                    |
|                                            | Responsible Data,<br>Information Systems<br>and Information Flows | How will we obtain data, and what are<br>our informational flows? How will we<br>appropriately limit the scope of our data<br>collection? How will we retain and delete<br>data as needed?                           | Govern 1.1<br>Govern 1.2<br>Govern 1.4<br>Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Govern 6.2<br>Map 1.1<br>Map 1.2<br>Map 2.3<br>Map 4.1<br>Measure 2.2<br>Measure 2.10<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1 |

*Continued*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                                                                                                                                   | Relevant NIST AI RMF<br>Subcategories                                                                                                                             |
|--------------------------------------------|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Safe                                       | Data Stability                   | How will we analyze and monitor for data<br>drift over time?                                                                                                                                                                                                                              | Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Govern 6.2<br>Map 2.3<br>Measure 2.7<br>Measure 3.1<br>Manage 3.2<br>Manage 4.1                                         |
| Fair with Harmful Bias<br>Managed          | Data Balance*                    | How will we assess and improve the<br>balance and diversity of the data? How<br>will we evaluate all data sets for inclusion<br>and representation of demographic<br>groups? How will we guard against proxies<br>for demographic information that could<br>contribute to discrimination? | Govern 3.1<br>Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Map 1.1<br>Map 1.2<br>Map 2.3<br>Measure 2.2<br>Measure 2.11<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1    |
| Secure and Resilient                       | Data Security                    | How will the security of data that is used for<br>training or created be ensured?                                                                                                                                                                                                         | Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Govern 6.2<br>Map 2.3<br>Map 4.1<br>Map 4.2<br>Measure 2.7<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1                    |
| Privacy-Enhanced                           | Data Protection*                 | How will we protect the data used to build<br>and operate the AI system? How will we use<br>encryption, differential privacy, federated<br>learning, data minimization, and/or other<br>best practices to protect data?                                                                   | Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Map 2.3<br>Map 4.1<br>Map 4.2<br>Measure 2.7<br>Measure 2.10<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1                  |
|                                            | Data Processing<br>Oversight*    | How will we establish data oversight<br>mechanisms, such as limiting and logging<br>data access?                                                                                                                                                                                          | Govern 4.3<br>Govern 5.1<br>Govern 6.1<br>Govern 6.2<br>Map 2.3<br>Map 4.1<br>Measure 2.10<br>Measure 2.8<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1<br>Manage 4.2 |

Manage 4.3

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                          | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                                                                               |
|--------------------------------------------|----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Consent to Use of Data*          | How will we enable people to consent to the<br>uses of their data?                                                                                                               | Govern 1.1<br>Govern 5.2<br>Map 4.1<br>Map 5.2<br>Measure 2.8<br>Manage 4.1<br>Manage 4.2<br>Manage 4.3                                                                                                                                             |
|                                            | Control of Use of Data*          | How will we ensure people have a say in how<br>information about them is used? How will we<br>honor the right to rectification and the right<br>to erasure?                      | Govern 1.1<br>Govern 5.2<br>Map 4.1<br>Map 5.2<br>Manage 4.1<br>Manage 4.2<br>Manage 4.3                                                                                                                                                            |
| Accountable and<br>Transparent             | Data Governance*                 | How will we analyze and follow data<br>governance practices for all intended uses,<br>stakeholders, and relevant geographic areas?<br>How will we ensure data rights and agency? | Govern 1.1<br>Govern 1.4<br>Govern 6.1<br>Govern 6.2<br>Map 1.1<br>Map 1.2<br>Map 1.3<br>Map 2.3<br>Map 4.1<br>Map 4.2<br>Map 5.1<br>Map 5.2<br>Measure 2.2<br>Measure 2.11<br>Measure 2.10<br>Manage 1.3<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1 |
|                                            | Traceable                        | How will we document the provenance of<br>data, processes, and artifacts involved in the<br>production of the AI system?                                                         | Govern 1.6<br>Govern 4.2<br>Map 1.1<br>Map 2.3<br>Map 4.1<br>Measure 2.1<br>Measure 2.2<br>Measure 2.8                                                                                                                                              |
| Responsible Practice<br>and Use            | Efficient Data Centers           | How can we make our use of data centers<br>more energy-efficient?                                                                                                                | Map 1.1<br>Measure 2.12                                                                                                                                                                                                                             |

# **AI LIFECYCLE STAGE: BUILD AND USE MODEL**

The purpose of the "build and use model" stage is to create, select, and train models or algorithms.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                                                                                                                                                                   | Relevant NIST AI RMF<br>Subcategories                                                                                                              |
|--------------------------------------------|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|
| Valid and Reliable                         | Accurate                         | How will we assess the accuracy of<br>what the model has learned using an<br>interpretation method (descriptive<br>accuracy)? How will we assess the<br>accuracy of the underlying data<br>relationships with the model (predictive<br>accuracy)? What benchmarks will we use?<br>How will we communicate this as needed? | Govern 4.3<br>Map 1.1<br>Map 2.2<br>Map 2.3<br>Measure 2.3<br>Measure 2.5<br>Manage 1.1<br>Manage 4.1                                              |
|                                            | Reproducible                     | How will we test whether desirable<br>outputs of the AI system can be<br>reproduced in different circumstances?                                                                                                                                                                                                           | Govern 4.3<br>Govern 5.1<br>Map 2.1<br>Map 2.2<br>Map 2.3<br>Measure 2.1<br>Measure 2.3<br>Measure 2.5<br>Manage 1.1                               |
|                                            | Efficient                        | How will we improve the efficiency of<br>the AI system in terms of its energy and<br>power usage, model size, and memory<br>consumption? How can we make the<br>model architecture of the AI system more<br>efficient?                                                                                                    | Govern 4.3<br>Map 2.1<br>Map 2.2<br>Map 2.3<br>Measure 2.3<br>Measure 2.4<br>Measure 2.5<br>Measure 2.12                                           |
| Safe                                       | Safely Interruptible             | How will we ensure that reliable technical<br>and procedural controls, including<br>deactivation and fail-safe shutdown, are<br>in place to enable the safe use of the AI<br>system?                                                                                                                                      | Govern 1.2<br>Govern 1.7<br>Govern 4.1<br>Govern 4.3<br>Govern 6.2<br>Map 1.6<br>Map 2.2<br>Measure 2.6<br>Measure 3.1<br>Manage 2.4<br>Manage 4.1 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness     | Question(s) to Consider                                                                                                                                                                                                                                                 | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                    |
|--------------------------------------------|--------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Loyal                                | To whom or what will the AI system be<br>"loyal," and will that be optimal and made<br>transparent?                                                                                                                                                                     | Govern 3.2<br>Govern 4.2<br>Govern 6.1<br>Map 1.1<br>Map 1.3<br>Map 2.1<br>Map 2.2<br>Map 2.3<br>Measure 1.3<br>Measure 2.4<br>Measure 2.8<br>Manage 4.1<br>Manage 4.3                   |
|                                            | Power-averse                         | How will we incentivize models to avoid<br>power or avoid gaining more power than<br>is necessary?                                                                                                                                                                      | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Map 1.1<br>Map 1.6<br>Map 2.3<br>Measure 2.6<br>Measure 3.1<br>Manage 2.4                                                                      |
|                                            | Containment                          | How can we contain the AI system to<br>prevent safety and security breaches?                                                                                                                                                                                            | Govern 1.7<br>Govern 4.3<br>Map 1.6<br>Map 2.2<br>Measure 2.6<br>Measure 2.7<br>Manage 2.4<br>Manage 4.1                                                                                 |
| Fair with Harmful Bias<br>Managed          | Mitigation of<br>Computational Bias* | How will we assess and mitigate<br>computational bias (including biased<br>input data and biased model design)?<br>How will we ensure the AI system does<br>not provide a lower quality of service for<br>certain demographic groups, including<br>marginalized groups? | Govern 1.1<br>Govern 1.2<br>Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 2.3<br>Map 5.2<br>Measure 1.3<br>Measure 2.2<br>Measure 2.11<br>Manage 4.1<br>Manage 4.3 |
| Secure and Resilient                       | Protection Against<br>Trojans        | How will we detect if there is hidden<br>functionality embedded in our models?                                                                                                                                                                                          | Govern 4.1<br>Govern 4.3<br>Map 2.3<br>Map 4.2<br>Measure 2.7<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1                                                                                  |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness              | Question(s) to Consider                                                                                                                                                                                        | Relevant NIST AI RMF<br>Subcategories                                                                                                |
|--------------------------------------------|-----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Built-in Defenses                             | How will the AI system respond to attacks<br>as they occur?                                                                                                                                                    | Govern 4.1<br>Govern 4.3<br>Map 2.3<br>Map 4.2<br>Measure 2.7<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1<br>Manage 4.3                |
| Explainable and<br>Interpretable           | Interpretable Uncertainty                     | How will we make model uncertainty<br>more interpretable by adding features<br>such as confidence interval outputs,<br>conditional probabilistic predictions<br>encoded through sentences, and<br>calibration? | Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 2.2<br>Measure 2.9                                                                           |
| Privacy-enhanced                           | Model Protection*                             | How will we protect model access that<br>could reveal sensitive information?                                                                                                                                   | Govern 1.1<br>Map 4.2<br>Measure 2.7<br>Measure 2.10<br>Manage 4.1                                                                   |
| Accountable and<br>Transparent             | System Honesty                                | How will we ensure the AI system only<br>presents outputs that are accurate and<br>not intentionally deceptive?                                                                                                | Govern 4.3<br>Map 1.1<br>Map 2.2<br>Map 2.3<br>Measure 2.3<br>Measure 2.4<br>Measure 2.5<br>Measure 2.6<br>Measure 2.9<br>Manage 4.1 |
| Responsible Practice<br>and Use            | Reduction of<br>Computational<br>Requirements | How can we reduce the computational<br>requirements of the AI system?                                                                                                                                          | Govern 1.2<br>Map 1.1<br>Map 3.2<br>Measure 2.12                                                                                     |

#### **AI LIFECYCLE STAGE: VERIFY AND VALIDATE**

The purpose of this is to verify and validate, calibrate, and interpret model output.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

|  | NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                                                 | Relevant NIST AI RMF<br>Subcategories                                                                                                                                       |
|--|--------------------------------------------|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  | Valid and Reliable                         | Verifiable                       | How will we verify that the system is<br>behaving as expected?                                                                                                                                          | Govern 4.3<br>Map 2.3<br>Measure 1.3<br>Measure 2.1<br>Measure 2.13<br>Manage 4.1                                                                                           |
|  |                                            | Reliable                         | How will we ensure the AI system<br>performs predictably and as intended,<br>including in new environments or with<br>new inputs? How will we determine<br>acceptable error rates for intended<br>uses? | Govern 4.3<br>Map 1.1<br>Map 2.2<br>Map 2.3<br>Measure 2.5<br>Manage 3.1<br>Manage 4.1                                                                                      |
|  |                                            | Replayable                       | How can we replay the behavior of<br>the system to see if the same input<br>generates the same output?                                                                                                  | Govern 4.3<br>Map 2.3<br>Measure 2.4<br>Measure 2.5<br>Manage 4.1                                                                                                           |
|  |                                            | Effective                        | How will we judge sufficient<br>effectiveness of the AI system, in the lab<br>and in the real world?                                                                                                    | Govern 4.3<br>Map 1.1<br>Map 1.2<br>Map 1.3<br>Map 2.2<br>Map 2.3<br>Map 3.1<br>Map 3.2<br>Map 5.2<br>Measure 2.5<br>Measure 4.2<br>Measure 4.3<br>Manage 1.1<br>Manage 4.1 |
|  |                                            | Valid                            | How will we validate the outputs of the<br>AI system, including through external<br>validation?                                                                                                         | Govern 4.3<br>Govern 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 2.5<br>Measure 3.3<br>Measure 4.2<br>Manage 1.1<br>Manage 4.1                                                 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                           | Question(s) to Consider                                                                                                                                                                                   | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                     |
|--------------------------------------------|------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Appropriate Capabilities<br>for the Tasks                  | How will we review whether the<br>capabilities of the AI system are<br>appropriate for a particular use and<br>context?                                                                                   | Govern 4.3<br>Map 1.1<br>Map 1.2<br>Map 1.3<br>Map 2.1<br>Map 2.2<br>Map 2.3<br>Measure 1.3<br>Measure 2.4<br>Measure 2.5<br>Manage 4.1                                                   |
|                                            | Appropriate System<br>Design and Training for<br>the Tasks | How will we review that the design and<br>training of the system is appropriate<br>for intended and likely uses, and is not<br>underspecified?                                                            | Govern 4.3<br>Map 1.1<br>Map 1.3<br>Map 2.1<br>Map 2.2<br>Map 2.3<br>Map 3.3<br>Measure 2.3<br>Measure 2.4<br>Measure 2.5<br>Manage 4.1                                                   |
| Safe                                       | Protection from Proxy<br>Gaming                            | How will we test the ability of the AI<br>system to try to "game" a proxy of a<br>true objective function, or to learn<br>novel methods to achieve its objective<br>function? How will this be prevented? | Govern 4.3<br>Map 1.6<br>Map 2.2<br>Map 2.3<br>Measure 2.6<br>Measure 3.1<br>Manage 4.1<br>Manage 4.3                                                                                     |
|                                            | Review                                                     | How will we review any errors or<br>inconsistencies with the AI system that<br>emerge?                                                                                                                    | Govern 4.3<br>Measure 1.3<br>Measure 2.6<br>Measure 3.1<br>Manage 3.1<br>Manage 4.1<br>Manage 4.3                                                                                         |
| Fair with Harmful Bias<br>Managed          | Non-Discrimination*                                        | How will we ensure the AI system is not<br>discriminatory across gender, racial,<br>ability, age, political beliefs, religion, or<br>other dimensions?                                                    | Govern 1.1<br>Govern 1.2<br>Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 2.2<br>Measure 2.11<br>Measure 3.3<br>Manage 4.1 |

| NIST Characteristics of        | Properties of                                                         | Question(s) to Consider                                                                                                                                                                                                                                                                         | Relevant NIST AI RMF                                                                                                                                 |
|--------------------------------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Trustworthiness                | Trustworthiness                                                       |                                                                                                                                                                                                                                                                                                 | Subcategories                                                                                                                                        |
| Secure and Resilient           | Robust                                                                | How will we protect the AI system<br>against cyber attacks, adversarial<br>attacks, data poisoning, model leakage,<br>evasion, inversion, etc., and ensure<br>ongoing performance? How will<br>we ensure the system is robust to<br>optimizers that aim to induce specific<br>system responses? | Govern 4.1<br>Govern 4.3<br>Govern 6.1<br>Govern 6.2<br>Map 2.3<br>Measure 2.7<br>Manage 2.4<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1<br>Manage 4.3 |
|                                | Resilient                                                             | How will we assess the AI system's<br>ability to handle uncertainty and<br>unknown environments?                                                                                                                                                                                                | Govern 4.1<br>Govern 4.3<br>Govern 6.1<br>Map 1.1<br>Map 2.2<br>Map 2.3<br>Measure 2.5<br>Measure 2.7<br>Measure 3.1<br>Manage 4.1<br>Manage 4.3     |
| Privacy-Enhanced               | Protection from<br>Unwarranted Data<br>Access*                        | How will we ensure the AI system<br>cannot be used to give unwarranted<br>access to data?                                                                                                                                                                                                       | Govern 1.1<br>Govern 4.3<br>Govern 6.1<br>Map 2.3<br>Measure 2.7<br>Measure 2.10<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1<br>Manage 4.3             |
| Accountable and<br>Transparent | Future Projections of<br>Possible System and<br>Environmental Changes | How might the AI system learn and<br>evolve over time? How might the<br>environment it is deployed in change<br>over time?                                                                                                                                                                      | Govern 1.5<br>Govern 4.3<br>Map 1.1<br>Map 3.3<br>Map 5.1<br>Measure 2.8<br>Measure 3.1<br>Measure 3.2<br>Measure 3.3<br>Manage 2.3<br>Manage 4.1    |

#### **AI LIFECYCLE STAGE: DEPLOY AND USE**

The purpose of the deploy and use stage is to pilot, check compatibility with legacy systems, verify regulatory compliance, manage organizational change, and evaluate user experience.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                                             | Question(s) to Consider                                                                                                                                                                                                                                                          | Relevant NIST AI RMF<br>Subcategories                                                                                   |
|--------------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| Valid and Reliable                         | Generalizable                                                                | How will we ensure that the AI system<br>can generalize from the testing<br>environment to the complexity or<br>different context of the application<br>environment?                                                                                                             | Govern 4.3<br>Map 1.1<br>Map 1.3<br>Map 2.2<br>Map 3.3<br>Measure 2.5<br>Manage 1.1<br>Manage 4.1                       |
|                                            | Effective Assessment<br>of the Complexity<br>of Networks and<br>Dependencies | How will we assess the complexity of<br>integrated networks and dependencies<br>required for the functioning of the AI<br>system?                                                                                                                                                | Govern 2.1<br>Govern 3.2<br>Govern 6.1<br>Map 1.1<br>Map 4.1<br>Manage 3.1                                              |
|                                            | Usable*                                                                      | How will we test the usability of the<br>AI system for all kinds of users and<br>facilitate user feedback? How will the<br>user interface be tested for usability,<br>comprehension, and other attributes?<br>How will we ensure users know how to<br>interpret system behavior? | Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 1.3<br>Measure 2.9<br>Measure 3.3<br>Manage 4.2                                 |
| Safe                                       | Effective Detection of<br>Anomalies                                          | How will we detect potential novel<br>hazards?                                                                                                                                                                                                                                   | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Map 2.2<br>Map 2.3<br>Measure 2.6<br>Measure 2.7<br>Measure 3.1<br>Manage 4.1 |
| Fair with Harmful Bias<br>Managed          | Accessible*                                                                  | How will we ensure that the AI system's<br>user interface is usable by those with<br>special needs or disabilities, or those at<br>risk of exclusion?                                                                                                                            | Govern 1.1<br>Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.2<br>Manage 4.1                     |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                    | Question(s) to Consider                                                                                                                                                                                                                                                                                              | Relevant NIST AI RMF<br>Subcategories                                                                       |
|--------------------------------------------|-----------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Secure and Resilient                       | Use of Adversarial Testing                          | How will we establish "bug bounties" and<br>enable "red teams" to try to deliberately<br>find vulnerabilities in the AI system?                                                                                                                                                                                      | Govern 4.1<br>Govern 4.3<br>Govern 5.1<br>Govern 5.2<br>Map 2.3<br>Measure 2.7<br>Measure 3.1<br>Manage 4.1 |
| Explainable and<br>Interpretable           | Interpretable*                                      | How will we judge the interpretability of<br>the system's explanation to the particular<br>context and user?                                                                                                                                                                                                         | Govern 5.2<br>Map 1.1<br>Measure 2.9<br>Manage 4.1                                                          |
| Accountable and<br>Transparent             | Responsible Publication<br>and Disclosure           | How will we assess potential risks of<br>publicizing, publishing, opening up<br>for external use, or open-sourcing an<br>AI system's code or model? How will<br>we determine a strategy to safely and<br>appropriately release the AI system, and<br>what protections may be necessary to<br>prevent harm or misuse? | Govern 1.2<br>Govern 4.1<br>Map 1.1<br>Measure 2.6<br>Measure 2.8<br>Manage 4 .1                            |
|                                            | Information-sharing                                 | How will we share critical information<br>about our AI system with relevant<br>authorities and stakeholders?                                                                                                                                                                                                         | Govern 1.1<br>Govern 1.4<br>Govern 4.2<br>Govern 4.3<br>Measure 1.3<br>Measure 2.8<br>Manage 4.3            |
|                                            | User Testing and<br>Engagement; User<br>Experience* | How will we test the system with<br>users, and how will we engage them<br>in iterating upon the system design<br>and deployment? How will we test and<br>improve the user experience?                                                                                                                                | Govern 5.1<br>Govern 5.2<br>Map 5.2<br>Measure 3.3<br>Measure 4.1<br>Manage 4.1                             |
|                                            | Proactive<br>Communication*                         | How can we inform users that they are<br>interacting with an AI system (and what<br>type of AI system), or that a decision that<br>impacts them was made by an AI system,<br>and how can we provide expectations as<br>to the system's capabilities, benefits, and<br>limitations and potential risks?               | Govern 1.1<br>Measure 2.8<br>Manage 4.1<br>Manage 4.3                                                       |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                   | Relevant NIST AI RMF<br>Subcategories                                                                                                 |
|--------------------------------------------|----------------------------------|---------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|
| Responsible Practice<br>and Use            | Beneficial to Society            | How will we ensure the AI system will be<br>leveraged to benefit society? | Govern 1.2<br>Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 3.1<br>Map 3.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3 |
|                                            |                                  |                                                                           | Measure 3.3<br>Measure 4.2<br>Measure 4.3<br>Manage 1.1<br>Manage 4.1                                                                 |

#### **AI LIFECYCLE STAGE: OPERATE AND MONITOR**

The purpose of this stage is to operate the AI system and continuously assess its recommendations and impacts (both intended and unintended) in light of objectives and ethical considerations.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness | Question(s) to Consider                                                                                                                                                                                                                                                               | Relevant NIST AI<br>RMF Subcategories                                                                                                  |
|--------------------------------------------|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| Valid and Reliable                         | Continuous Monitoring            | How will we monitor the AI system's capabilities,<br>outputs, errors, breaches, success, and impacts<br>over time, especially for self-learning or<br>continuous-learning AI systems? How will we<br>determine which events to monitor, and how to<br>prioritize review and response? | Govern 4.1<br>Govern 4.2<br>Govern 4.3<br>Map 5.2<br>Measure 2.4<br>Measure 3.1<br>Manage 3.1<br>Manage 3.2<br>Manage 4.1              |
|                                            | Maintaining Quality<br>Over Time | How will we ensure the maintainability of the<br>AI system after it is operationalized? How will<br>we maintain the quality of the system and its<br>outputs over time?                                                                                                               | Govern 4.3<br>Map 5.2<br>Measure 2.4<br>Measure 3.3<br>Measure 4.3<br>Manage 2.2<br>Manage 4.1<br>Manage 4.2                           |
|                                            | Acceptable and<br>Desirable      | How will we judge the acceptability and<br>desirability of the use of the AI system by the<br>communities, organizations, and institutions that<br>are using the system and are impacted by it?                                                                                       | Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Measure 4.2<br>Measure 4.3<br>Manage 1.1<br>Manage 4.1 |
|                                            | Human Agency                     | How will human agency be meaningfully<br>incorporated in the operation of the AI system?                                                                                                                                                                                              | Govern 2.1<br>Govern 3.2<br>Govern 5.2<br>Map 1.1<br>Map 2.2<br>Measure 1.3<br>Measure 2.2<br>Measure 3.3<br>Measure 4.3<br>Manage 4.1 |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness       | Question(s) to Consider                                                                                                                                                                                                                                     | Relevant NIST AI<br>RMF Subcategories                                                                                                                                                |
|--------------------------------------------|----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Human Control                          | How will we ensure that a human is in control<br>or meaningfully in the loop of the operational<br>decision-making process of the AI system, and<br>has been trained to exercise oversight and avoid<br>overconfidence in the system?                       | Govern 2.1<br>Govern 3.2<br>Govern 4.3<br>Map 1.1<br>Map 2.2<br>Map 3.4<br>Map 3.5<br>Measure 1.2<br>Measure 3.3<br>Manage 2.4<br>Manage 4.1                                         |
|                                            | Human Oversight                        | How will human oversight be ensured in<br>the operation of the AI system? How will we<br>designate and train the stakeholders responsible<br>for managing and monitoring the AI system,<br>including overriding or interrupting the system<br>if necessary? | Govern 2.1<br>Govern 3.2<br>Govern 4.3<br>Map 1.1<br>Map 2.2<br>Map 3.4<br>Map 3.5<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Measure 4.2<br>Manage 4.1<br>Manage 4.3               |
|                                            | Appropriate Retirement                 | How will we determine when and how to retire<br>the use of the AI system?                                                                                                                                                                                   | Govern 1.7<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Manage 2.4<br>Manage 4.1                                                                                                      |
|                                            | Iterative Learning and<br>Improvements | How will we continue to learn, iterate, and<br>improve over time?                                                                                                                                                                                           | Govern 1.5<br>Govern 2.2<br>Govern 3.1<br>Govern 4.1<br>Govern 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Measure 4.1<br>Measure 4.2<br>Measure 4.3<br>Manage 4.1<br>Manage 4.2 |
| Safe                                       | Re-evaluation                          | How will we evaluate when the AI system has<br>been sufficiently modified such that a new<br>review of its technical robustness and safety is<br>warranted?                                                                                                 | Govern 4.3<br>Measure 1.3<br>Measure 2.6<br>Measure 3.1<br>Manage 2.3<br>Manage 4.1                                                                                                  |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                            | Question(s) to Consider                                                                                                                                                                                                                   | Relevant NIST AI<br>RMF Subcategories                                                                                                                                              |
|--------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|                                            | Assurance /<br>Management of<br>Continual Learning          | How will we assess shifts to an AI system<br>if it learns and evolves over time, including<br>the possibility of emerging properties or<br>discontinuous jumps in capabilities?                                                           | Govern 4.1<br>Govern 4.3<br>Map 2.2<br>Measure 2.6<br>Measure 3.1<br>Manage 2.3<br>Manage 4.1                                                                                      |
|                                            | Awareness of Functional<br>Evolution                        | How will we track shifts in the AI system's<br>functionality over time?                                                                                                                                                                   | Govern 4.1<br>Govern 4.3<br>Map 2.2<br>Measure 2.4<br>Measure 2.6<br>Measure 3.1<br>Manage 2.3<br>Manage 4.1                                                                       |
|                                            | Assurance /<br>Management<br>of Emergent<br>Functionalities | How will we predict and detect new capabilities<br>and goals of the AI system?                                                                                                                                                            | Govern 4.1<br>Govern 4.3<br>Map 2.2<br>Measure 2.4<br>Measure 2.6<br>Measure 3.1<br>Manage 2.3<br>Manage 4.1                                                                       |
| Fair with Harmful Bias<br>Managed          | Shared Benefit                                              | How will the benefits of the AI system's use be<br>distributed? Can those benefits be shared more<br>widely?                                                                                                                              | Govern 3.1<br>Govern 5.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 3.1<br>Manage 2.2<br>Manage 4.2                                                                                |
| Accountable and<br>Transparent             | Auditable                                                   | How will independent auditors or an<br>independent monitoring body be able to assess<br>the AI system and its impacts? Is there sufficient<br>documentation to support an audit?                                                          | Govern 1.4<br>Govern 4.2<br>Map 4.1<br>Map 5.1<br>Measure 1.3<br>Measure 2.8<br>Manage 4.3                                                                                         |
| Responsible Practice<br>and Use            | Prevention of Significant<br>Adverse Impacts                | How will we identify and prevent or mitigate<br>and minimize significant adverse impacts,<br>including harm and/or violence to people or<br>communities, including harassment, stereotyping<br>or demeaning, addiction, or over-reliance? | A majority of all of<br>the subcategories<br>are critical. Map 1.1<br>is especially relevant<br>to help understand<br>the purpose,<br>context, and impacts<br>of the intended use. |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                           | Question(s) to Consider                                                                                                                                                                                                         | Relevant NIST AI<br>RMF Subcategories                                                                                       |
|--------------------------------------------|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|
|                                            | Prevention of Malicious<br>or Harmful Synthetic<br>Content | How will we monitor and prevent or mitigate<br>the creation or spread of malicious or harmful<br>synthetic content, such as non-consensual<br>deepfakes?                                                                        | Govern 1.1<br>Govern 1.2<br>Map 1.1<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Manage 2.4<br>Manage 4.1         |
|                                            | Prevention of Misuses<br>and Abuses                        | How will we monitor uses and actively prevent<br>or mitigate misuses and abuses, including human<br>rights abuses? For example, how will we prevent<br>the sale or the system to actors with records of<br>human rights abuses? | Govern 1.1<br>Govern 1.2<br>Map 1.1<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Manage 2.4<br>Manage 4.1         |
|                                            | Prevention of Social or<br>Behavioral Manipulation         | How will we monitor and prevent or mitigate<br>individual or social manipulation, for example<br>through recommender systems, dark patterns,<br>or computational propaganda?                                                    | Govern 1.1<br>Govern 1.2<br>Map 1.1<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Manage 2.4<br>Manage 4.1         |
|                                            | Assessment of<br>Environmental<br>Implications             | How will we analyze and document the<br>environmental implications of the AI system and<br>its uses?                                                                                                                            | Govern 3.1<br>Govern 4.2<br>Map 1.1<br>Map 3.2<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 2.12<br>Manage 4.1           |
|                                            | Oversight of Third-Party<br>Uses                           | How will we determine which third parties<br>to do business with, and how will we oversee<br>third-party uses to help prevent misuses of the<br>AI system?                                                                      | Govern 6.1<br>Govern 6.2<br>Map 4.1<br>Map 4.2<br>Manage 3.1<br>Manage 3.2                                                  |
|                                            | Assessment of<br>Implications Over Time                    | How will we assess the implications of the use<br>of the AI system over time? What events should<br>trigger reevaluation, and how frequently should<br>we reevaluate?                                                           | Govern 1.5<br>Govern 4.2<br>Map 1.1<br>Measure 1.2<br>Measure 1.3<br>Measure 3.1<br>Measure 3.3<br>Manage 2.3<br>Manage 4.1 |

# **AI LIFECYCLE STAGE: USE OR IMPACTED BY**

The purpose of this stage is to use the system or technology, monitor and assess its impacts, seek mitigation of impacts, and advocate for rights.

*Properties followed by an asterisk may be less relevant for AI systems that are not humanfacing, meaning they do not engage directly with human users or operators, make use of human data, or inform human decision-making.*

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness        | Question(s) to Consider                                                                                                                                     | Relevant NIST AI RMF<br>Subcategories                                                                        |
|--------------------------------------------|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| Valid and Reliable                         | Engagement with<br>Impacted Communities | How will we identify and engage with<br>communities impacted by the use of the<br>system, either directly or indirectly, and<br>incorporate their feedback? | Govern 5.1<br>Govern 5.2<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Measure 4.3<br>Manage 4.1<br>Manage 4.3 |
|                                            | Effective Feedback*                     | How will we establish a dedicated channel<br>for feedback and questions about the AI<br>system from users and the general public?                           | Govern 5.1<br>Govern 5.2<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Manage 4.1                              |
| Safe                                       | Incident Reporting                      | How will we publicly report incidents and<br>adverse impacts of the AI system, such<br>as mistakes, errors, breaches, unintended<br>consequences, etc.?     | Govern 4.2<br>Govern 4.3<br>Measure 2.6<br>Manage 4.1<br>Manage 4.3                                          |
| Fair with Harmful Bias<br>Managed          | Fair Access to AI Tools<br>and Services | How can we promote widespread and<br>equitable access to our AI tools and services,<br>and any resources or opportunities they<br>enable?                   | Govern 3.1<br>Govern 5.2<br>Map 1.1<br>Map 1.2<br>Map 5.2<br>Manage 4.2                                      |
| Secure and Resilient                       | Vulnerability Disclosure                | How will we establish a coordinated policy to<br>encourage responsible vulnerability research<br>and disclosure?                                            | Govern 1.1<br>Govern 1.2<br>Govern 4.2<br>Govern 4.3<br>Map 5.2<br>Measure 2.7<br>Manage 4.1<br>Manage 4.3   |
| Explainable and<br>Interpretable           | Relevant Explanation                    | How will we judge how informative and<br>relevant a system's explanation is to the<br>particular context and user?                                          | Govern 5.2<br>Map 5.2<br>Measure 2.9<br>Manage 4.2<br>Manage 4.3                                             |

| NIST Characteristics of<br>Trustworthiness | Properties of<br>Trustworthiness                      | Question(s) to Consider                                                                                                                       | Relevant NIST AI RMF<br>Subcategories                                                                                                                                                |
|--------------------------------------------|-------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Privacy-enhanced                           | Effective Notification*                               | How will we notify users and impacted<br>communities about privacy or security<br>breaches, or other incidents?                               | Govern 1.1<br>Govern 4.3<br>Map 5.2<br>Measure 2.8<br>Manage 2.3<br>Manage 4.1<br>Manage 4.3                                                                                         |
| Accountable and<br>Transparent             | Facilitation of<br>Contestability*                    | How will users be able to contest or appeal a<br>decision or action made by the AI system?                                                    | Govern 5.2<br>Map 5.2<br>Measure 3.3<br>Manage 4.1                                                                                                                                   |
|                                            | Facilitation of Redress or<br>Recourse                | How will we support or compensate people<br>who are negatively affected by the use of the<br>AI system?                                       | Govern 5.2<br>Map 5.2<br>Measure 3.3<br>Manage 4.1<br>Manage 4.3                                                                                                                     |
|                                            | Engagement with<br>Global Governance<br>Deliberations | How will we analyze, follow, and engage in<br>relevant global governance deliberations and<br>practices related to artificial intelligence?   | Govern 1.1<br>Govern 1.2<br>Govern 5.1<br>Map 5.2<br>Measure 2.8                                                                                                                     |
|                                            | Data and System<br>Accessibility                      | How can we enable access to the AI system<br>and datasets to relevant authorities,<br>independent researchers, and trusted<br>intermediaries? | Govern 4.2<br>Govern 5.1<br>Map 1.2<br>Map 5.2<br>Measure 2.8<br>Manage 4.2                                                                                                          |
|                                            | Informed Consent of<br>Use*                           | How will we enable users of the AI system to<br>consent to its use? How will we enable them<br>to withdraw consent?                           | Govern 5.2<br>Map 5.2<br>Measure 2.2<br>Manage 4.1                                                                                                                                   |
| Responsible Practice<br>and Use            | Ability to Opt Out*                                   | How will we ensure that people have specific<br>and clear opportunities to opt out of use of<br>the AI system?                                | Map 5.2<br>Measure 2.2<br>Manage 4.1                                                                                                                                                 |
|                                            | Consumer Protection*                                  | How will we protect consumers or users of<br>the system from harm?                                                                            | Govern 1.1<br>Govern 4.1<br>Govern 4.3<br>Govern 5.1<br>Map 1.1<br>Map 3.4<br>Map 3.5<br>Map 5.1<br>Map 5.2<br>Measure 1.3<br>Measure 3.3<br>Measure 4.1<br>Manage 4.1<br>Manage 4.3 |
|                                            | Due Process and<br>Protection                         | How will we protect whistleblowers, NGOs,<br>trade unions, or other entities who come<br>forward with concerns about the AI system?           | Govern 1.1<br>Map 4.1<br>Measure 3.3<br>Manage 4.1                                                                                                                                   |

# Acknowledgments

This paper would not have been possible without the support and vision of Claire Vishik and Amit Elazari, who served as partners on this project from Intel Corporation alongside the Center for Long-Term Cybersecurity (CLTC).

The author is also deeply grateful to William Mullen and Xiangyu Yue for their invaluable research contributions, and to the many individuals who shared their expertise, provided feedback, and participated in the workshop held in July 2022, "Properties of Trustworthiness for Artificial Intelligence", including McKane Andrus, Luis Aranda, Rachel Azafrani, Anthony Barrett, Haydn Belfield, Rosie Campbell, Ria Cheruvu, Corrine Elliott, Jordan Famularo, Dan Hendrycks, Zoe Kahn, Yolanda Lannquist, Richard Mallah, Emily McReynolds, Deirdre Mulligan, Dawn Nafus, Rachel Nico, Brandie Nonnecke, Ifejesu Ogunleye, Kate Perkins, Karine Perset, Valerie Pilloud, Darya Pilram, Mario Romao, Irene Solaiman, Narayan Srinivasa, Jonathan Stray, Elham Tabassi, Jun Takei, Apostol Vassilev, Sarah Villeneuve, Russell Wald, Richmond Wong, Roberto Zicari, and Polina Zvyagina.

Special thanks also to Ann Cleaveland and Chuck Kapelke of CLTC for their guidance and editing, Rachel Wesen and Matt Nagamine for event support, and to Nicole Hayward for her expert design and formatting of this paper.

This project was made possible by a gift from Intel in support of independent academic research.

# About the Author

**Jessica Newman** is the Director of the AI Security Initiative, housed at the UC Berkeley Center for Long-Term Cybersecurity. She is also the Co-Director of the UC Berkeley AI Policy Hub and Co-Director of the Algorithmic Fairness and Opacity Group (AFOG). Her work focuses on the governance, policy, and global security implications of artificial intelligence. She serves as a member of the OECD Expert Group on AI Risk & Accountability and the IEEE Working Group on Recommended Practice for Organizational Governance of Artificial Intelligence.

![](_page_37_Picture_0.jpeg)

**UC BERKELEY**

**CENTER FOR LONG-TERM CYBERSECURITY**

UC Berkeley

Center for Long-Term Cybersecurity | <cltc.berkeley.edu> | @CLTCBerkeley